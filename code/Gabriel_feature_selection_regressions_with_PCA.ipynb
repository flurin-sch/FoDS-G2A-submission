{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as sts\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from sklearn.model_selection import KFold, GridSearchCV, StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DepMap_ID</th>\n",
       "      <th>cell_line_name</th>\n",
       "      <th>stripped_cell_line_name</th>\n",
       "      <th>CCLE_Name</th>\n",
       "      <th>alias</th>\n",
       "      <th>COSMICID</th>\n",
       "      <th>sex</th>\n",
       "      <th>source</th>\n",
       "      <th>RRID</th>\n",
       "      <th>WTSI_Master_Cell_ID</th>\n",
       "      <th>...</th>\n",
       "      <th>lineage_sub_subtype</th>\n",
       "      <th>lineage_molecular_subtype</th>\n",
       "      <th>default_growth_pattern</th>\n",
       "      <th>model_manipulation</th>\n",
       "      <th>model_manipulation_details</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>parent_depmap_id</th>\n",
       "      <th>Cellosaurus_NCIt_disease</th>\n",
       "      <th>Cellosaurus_NCIt_id</th>\n",
       "      <th>Cellosaurus_issues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACH-000016</td>\n",
       "      <td>SLR 21</td>\n",
       "      <td>SLR21</td>\n",
       "      <td>SLR21_KIDNEY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Academic lab</td>\n",
       "      <td>CVCL_V607</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PT-JnARLB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Clear cell renal cell carcinoma</td>\n",
       "      <td>C4033</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACH-000032</td>\n",
       "      <td>MHH-CALL-3</td>\n",
       "      <td>MHHCALL3</td>\n",
       "      <td>MHHCALL3_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>DSMZ</td>\n",
       "      <td>CVCL_0089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>b_cell</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PT-p2KOyI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Childhood B acute lymphoblastic leukemia</td>\n",
       "      <td>C9140</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACH-000033</td>\n",
       "      <td>NCI-H1819</td>\n",
       "      <td>NCIH1819</td>\n",
       "      <td>NCIH1819_LUNG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>Academic lab</td>\n",
       "      <td>CVCL_1497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NSCLC_adenocarcinoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PT-9p1WQv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lung adenocarcinoma</td>\n",
       "      <td>C3512</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACH-000043</td>\n",
       "      <td>Hs 895.T</td>\n",
       "      <td>HS895T</td>\n",
       "      <td>HS895T_FIBROBLAST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>ATCC</td>\n",
       "      <td>CVCL_0993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2D: adherent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PT-rTUVZQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Melanoma</td>\n",
       "      <td>C3224</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACH-000049</td>\n",
       "      <td>HEK TE</td>\n",
       "      <td>HEKTE</td>\n",
       "      <td>HEKTE_KIDNEY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Academic lab</td>\n",
       "      <td>CVCL_WS59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immortalized</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PT-qWYYgr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No information is available about this cell li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>ACH-002393</td>\n",
       "      <td>CRO-AP3</td>\n",
       "      <td>CROAP3</td>\n",
       "      <td>CROAP3_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>Sanger</td>\n",
       "      <td>CVCL_1810</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>b_cell_primary_effusion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PT-TC0lZM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Primary effusion lymphoma</td>\n",
       "      <td>C6915</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1836</th>\n",
       "      <td>ACH-002394</td>\n",
       "      <td>GEO</td>\n",
       "      <td>GEO</td>\n",
       "      <td>GEO_LARGE_INTESTINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sanger</td>\n",
       "      <td>CVCL_0271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PT-Fa1q9q</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Colon carcinoma</td>\n",
       "      <td>C4910</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>ACH-002395</td>\n",
       "      <td>HuH-6 Clone 5</td>\n",
       "      <td>HUH6CLONE5</td>\n",
       "      <td>HUH6CLONE5_LIVER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>Sanger</td>\n",
       "      <td>CVCL_1296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PT-TtIXsL</td>\n",
       "      <td>ACH-000671</td>\n",
       "      <td>Hepatoblastoma</td>\n",
       "      <td>C3728</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1838</th>\n",
       "      <td>ACH-002396</td>\n",
       "      <td>Sarc9371</td>\n",
       "      <td>SARC9371</td>\n",
       "      <td>SARC9371_BONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sanger</td>\n",
       "      <td>CVCL_5G89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PT-715FdC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Osteosarcoma</td>\n",
       "      <td>C9145</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839</th>\n",
       "      <td>ACH-002397</td>\n",
       "      <td>KMH-2</td>\n",
       "      <td>KMHDASH2</td>\n",
       "      <td>KMH2_THYROID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2054094.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CVCL_S641</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PT-ix8RHJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thyroid gland anaplastic carcinoma</td>\n",
       "      <td>C3878</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1840 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       DepMap_ID cell_line_name stripped_cell_line_name  \\\n",
       "0     ACH-000016         SLR 21                   SLR21   \n",
       "1     ACH-000032     MHH-CALL-3                MHHCALL3   \n",
       "2     ACH-000033      NCI-H1819                NCIH1819   \n",
       "3     ACH-000043       Hs 895.T                  HS895T   \n",
       "4     ACH-000049         HEK TE                   HEKTE   \n",
       "...          ...            ...                     ...   \n",
       "1835  ACH-002393        CRO-AP3                  CROAP3   \n",
       "1836  ACH-002394            GEO                     GEO   \n",
       "1837  ACH-002395  HuH-6 Clone 5              HUH6CLONE5   \n",
       "1838  ACH-002396       Sarc9371                SARC9371   \n",
       "1839  ACH-002397          KMH-2                KMHDASH2   \n",
       "\n",
       "                                        CCLE_Name alias   COSMICID     sex  \\\n",
       "0                                    SLR21_KIDNEY   NaN        NaN     NaN   \n",
       "1     MHHCALL3_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE   NaN        NaN  Female   \n",
       "2                                   NCIH1819_LUNG   NaN        NaN  Female   \n",
       "3                               HS895T_FIBROBLAST   NaN        NaN  Female   \n",
       "4                                    HEKTE_KIDNEY   NaN        NaN     NaN   \n",
       "...                                           ...   ...        ...     ...   \n",
       "1835    CROAP3_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE   NaN        NaN    Male   \n",
       "1836                          GEO_LARGE_INTESTINE   NaN        NaN     NaN   \n",
       "1837                             HUH6CLONE5_LIVER   NaN        NaN    Male   \n",
       "1838                                SARC9371_BONE   NaN        NaN     NaN   \n",
       "1839                                 KMH2_THYROID   NaN  2054094.0    Male   \n",
       "\n",
       "            source       RRID  WTSI_Master_Cell_ID  ...  \\\n",
       "0     Academic lab  CVCL_V607                  NaN  ...   \n",
       "1             DSMZ  CVCL_0089                  NaN  ...   \n",
       "2     Academic lab  CVCL_1497                  NaN  ...   \n",
       "3             ATCC  CVCL_0993                  NaN  ...   \n",
       "4     Academic lab  CVCL_WS59                  NaN  ...   \n",
       "...            ...        ...                  ...  ...   \n",
       "1835        Sanger  CVCL_1810                  NaN  ...   \n",
       "1836        Sanger  CVCL_0271                  NaN  ...   \n",
       "1837        Sanger  CVCL_1296                  NaN  ...   \n",
       "1838        Sanger  CVCL_5G89                  NaN  ...   \n",
       "1839           NaN  CVCL_S641                  NaN  ...   \n",
       "\n",
       "          lineage_sub_subtype lineage_molecular_subtype  \\\n",
       "0                         NaN                       NaN   \n",
       "1                      b_cell                       NaN   \n",
       "2        NSCLC_adenocarcinoma                       NaN   \n",
       "3                         NaN                       NaN   \n",
       "4                         NaN                       NaN   \n",
       "...                       ...                       ...   \n",
       "1835  b_cell_primary_effusion                       NaN   \n",
       "1836                      NaN                       NaN   \n",
       "1837                      NaN                       NaN   \n",
       "1838                      NaN                       NaN   \n",
       "1839                      NaN                       NaN   \n",
       "\n",
       "     default_growth_pattern model_manipulation model_manipulation_details  \\\n",
       "0                       NaN                NaN                        NaN   \n",
       "1                       NaN                NaN                        NaN   \n",
       "2                       NaN                NaN                        NaN   \n",
       "3              2D: adherent                NaN                        NaN   \n",
       "4                       NaN       immortalized                        NaN   \n",
       "...                     ...                ...                        ...   \n",
       "1835                    NaN                NaN                        NaN   \n",
       "1836                    NaN                NaN                        NaN   \n",
       "1837                    NaN                NaN                        NaN   \n",
       "1838                    NaN                NaN                        NaN   \n",
       "1839                    NaN                NaN                        NaN   \n",
       "\n",
       "     patient_id parent_depmap_id                  Cellosaurus_NCIt_disease  \\\n",
       "0     PT-JnARLB              NaN           Clear cell renal cell carcinoma   \n",
       "1     PT-p2KOyI              NaN  Childhood B acute lymphoblastic leukemia   \n",
       "2     PT-9p1WQv              NaN                       Lung adenocarcinoma   \n",
       "3     PT-rTUVZQ              NaN                                  Melanoma   \n",
       "4     PT-qWYYgr              NaN                                       NaN   \n",
       "...         ...              ...                                       ...   \n",
       "1835  PT-TC0lZM              NaN                 Primary effusion lymphoma   \n",
       "1836  PT-Fa1q9q              NaN                           Colon carcinoma   \n",
       "1837  PT-TtIXsL       ACH-000671                            Hepatoblastoma   \n",
       "1838  PT-715FdC              NaN                              Osteosarcoma   \n",
       "1839  PT-ix8RHJ              NaN        Thyroid gland anaplastic carcinoma   \n",
       "\n",
       "     Cellosaurus_NCIt_id                                 Cellosaurus_issues  \n",
       "0                  C4033                                                NaN  \n",
       "1                  C9140                                                NaN  \n",
       "2                  C3512                                                NaN  \n",
       "3                  C3224                                                NaN  \n",
       "4                    NaN  No information is available about this cell li...  \n",
       "...                  ...                                                ...  \n",
       "1835               C6915                                                NaN  \n",
       "1836               C4910                                                NaN  \n",
       "1837               C3728                                                NaN  \n",
       "1838               C9145                                                NaN  \n",
       "1839               C3878                                                NaN  \n",
       "\n",
       "[1840 rows x 29 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_info = pd.read_csv('../data/sample_info.csv')\n",
    "radiosensitivity = pd.read_csv('../data/radiosensitivity.csv')\n",
    "expression = pd.read_csv('../data/expressionData.csv')\n",
    "sample_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 461 entries, 0 to 460\n",
      "Columns: 19227 entries, cell_line_name to AUC\n",
      "dtypes: float64(19226), object(1)\n",
      "memory usage: 67.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.merge(expression, radiosensitivity, on='cell_line_name', how='inner')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 461 entries, 0 to 460\n",
      "Columns: 19222 entries, TSPAN6 (7105) to SF2\n",
      "dtypes: float64(19222)\n",
      "memory usage: 67.6 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = data.drop(columns=['cell_line_name'])\n",
    "data = data.drop(columns=['R2', 'AUC', 'alpha', 'beta'])\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 461 entries, 0 to 460\n",
      "Columns: 19222 entries, TSPAN6 (7105) to SF2\n",
      "dtypes: float64(19222)\n",
      "memory usage: 67.6 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['SF2']\n",
    "X = data.drop(columns=['SF2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TSPAN6 (7105)</th>\n",
       "      <th>TNMD (64102)</th>\n",
       "      <th>DPM1 (8813)</th>\n",
       "      <th>SCYL3 (57147)</th>\n",
       "      <th>C1orf112 (55732)</th>\n",
       "      <th>FGR (2268)</th>\n",
       "      <th>CFH (3075)</th>\n",
       "      <th>FUCA2 (2519)</th>\n",
       "      <th>GCLC (2729)</th>\n",
       "      <th>NFYA (4800)</th>\n",
       "      <th>...</th>\n",
       "      <th>H3C2 (8358)</th>\n",
       "      <th>H3C3 (8352)</th>\n",
       "      <th>AC098582.1 (8916)</th>\n",
       "      <th>DUS4L-BCAP29 (115253422)</th>\n",
       "      <th>C8orf44-SGK3 (100533105)</th>\n",
       "      <th>ELOA3B (728929)</th>\n",
       "      <th>NPBWR1 (2831)</th>\n",
       "      <th>ELOA3D (100506888)</th>\n",
       "      <th>ELOA3 (162699)</th>\n",
       "      <th>CDR1 (1038)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.847496</td>\n",
       "      <td>5.251340</td>\n",
       "      <td>6.016585</td>\n",
       "      <td>2.454176</td>\n",
       "      <td>4.070389</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>5.404971</td>\n",
       "      <td>3.802193</td>\n",
       "      <td>5.289834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748461</td>\n",
       "      <td>0.124328</td>\n",
       "      <td>0.356144</td>\n",
       "      <td>1.761285</td>\n",
       "      <td>0.378512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226509</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.100978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.093180</td>\n",
       "      <td>2.548437</td>\n",
       "      <td>3.764474</td>\n",
       "      <td>0.275007</td>\n",
       "      <td>7.168321</td>\n",
       "      <td>6.881175</td>\n",
       "      <td>3.868884</td>\n",
       "      <td>4.542877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.863938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.750607</td>\n",
       "      <td>1.389567</td>\n",
       "      <td>0.111031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111031</td>\n",
       "      <td>0.111031</td>\n",
       "      <td>0.389567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.861955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.276683</td>\n",
       "      <td>1.819668</td>\n",
       "      <td>3.813525</td>\n",
       "      <td>0.176323</td>\n",
       "      <td>1.555816</td>\n",
       "      <td>6.204180</td>\n",
       "      <td>4.249445</td>\n",
       "      <td>4.078097</td>\n",
       "      <td>...</td>\n",
       "      <td>2.204767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.545968</td>\n",
       "      <td>2.498251</td>\n",
       "      <td>0.250962</td>\n",
       "      <td>0.014355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014355</td>\n",
       "      <td>0.014355</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.232661</td>\n",
       "      <td>1.176323</td>\n",
       "      <td>5.953032</td>\n",
       "      <td>2.632268</td>\n",
       "      <td>3.689299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056584</td>\n",
       "      <td>0.263034</td>\n",
       "      <td>5.366322</td>\n",
       "      <td>5.871597</td>\n",
       "      <td>...</td>\n",
       "      <td>1.214125</td>\n",
       "      <td>2.839960</td>\n",
       "      <td>0.286881</td>\n",
       "      <td>2.211012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.726831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.779260</td>\n",
       "      <td>1.475085</td>\n",
       "      <td>4.128458</td>\n",
       "      <td>0.014355</td>\n",
       "      <td>1.992768</td>\n",
       "      <td>6.685660</td>\n",
       "      <td>6.011898</td>\n",
       "      <td>4.761285</td>\n",
       "      <td>...</td>\n",
       "      <td>2.017922</td>\n",
       "      <td>1.378512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.555816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014355</td>\n",
       "      <td>0.084064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014355</td>\n",
       "      <td>0.111031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>5.723559</td>\n",
       "      <td>0.263034</td>\n",
       "      <td>6.433627</td>\n",
       "      <td>2.916477</td>\n",
       "      <td>3.372952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070389</td>\n",
       "      <td>5.743623</td>\n",
       "      <td>5.659925</td>\n",
       "      <td>3.121015</td>\n",
       "      <td>...</td>\n",
       "      <td>1.176323</td>\n",
       "      <td>0.275007</td>\n",
       "      <td>0.367371</td>\n",
       "      <td>1.367371</td>\n",
       "      <td>0.263034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>4.288359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.515227</td>\n",
       "      <td>2.163499</td>\n",
       "      <td>4.028569</td>\n",
       "      <td>0.226509</td>\n",
       "      <td>6.631977</td>\n",
       "      <td>7.699885</td>\n",
       "      <td>4.050502</td>\n",
       "      <td>4.051372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.678072</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.411426</td>\n",
       "      <td>1.970854</td>\n",
       "      <td>0.298658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>4.177918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.028348</td>\n",
       "      <td>2.073820</td>\n",
       "      <td>3.899176</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>0.238787</td>\n",
       "      <td>6.879338</td>\n",
       "      <td>6.059831</td>\n",
       "      <td>3.856986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.464668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.157044</td>\n",
       "      <td>2.589763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>4.276497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.454669</td>\n",
       "      <td>3.246408</td>\n",
       "      <td>3.715893</td>\n",
       "      <td>1.070389</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>6.489768</td>\n",
       "      <td>5.508746</td>\n",
       "      <td>4.440952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.948601</td>\n",
       "      <td>1.650765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>4.193772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.781884</td>\n",
       "      <td>2.769772</td>\n",
       "      <td>4.068671</td>\n",
       "      <td>0.097611</td>\n",
       "      <td>1.150560</td>\n",
       "      <td>5.752749</td>\n",
       "      <td>4.713146</td>\n",
       "      <td>4.155425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.498251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>461 rows × 19221 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     TSPAN6 (7105)  TNMD (64102)  DPM1 (8813)  SCYL3 (57147)  \\\n",
       "0         5.847496      5.251340     6.016585       2.454176   \n",
       "1         4.100978      0.000000     6.093180       2.548437   \n",
       "2         2.861955      0.000000     6.276683       1.819668   \n",
       "3         4.232661      1.176323     5.953032       2.632268   \n",
       "4         3.726831      0.000000     6.779260       1.475085   \n",
       "..             ...           ...          ...            ...   \n",
       "456       5.723559      0.263034     6.433627       2.916477   \n",
       "457       4.288359      0.000000     6.515227       2.163499   \n",
       "458       4.177918      0.000000     6.028348       2.073820   \n",
       "459       4.276497      0.000000     7.454669       3.246408   \n",
       "460       4.193772      0.000000     6.781884       2.769772   \n",
       "\n",
       "     C1orf112 (55732)  FGR (2268)  CFH (3075)  FUCA2 (2519)  GCLC (2729)  \\\n",
       "0            4.070389    0.042644    0.042644      5.404971     3.802193   \n",
       "1            3.764474    0.275007    7.168321      6.881175     3.868884   \n",
       "2            3.813525    0.176323    1.555816      6.204180     4.249445   \n",
       "3            3.689299    0.000000    0.056584      0.263034     5.366322   \n",
       "4            4.128458    0.014355    1.992768      6.685660     6.011898   \n",
       "..                ...         ...         ...           ...          ...   \n",
       "456          3.372952    0.000000    0.070389      5.743623     5.659925   \n",
       "457          4.028569    0.226509    6.631977      7.699885     4.050502   \n",
       "458          3.899176    0.042644    0.238787      6.879338     6.059831   \n",
       "459          3.715893    1.070389    0.344828      6.489768     5.508746   \n",
       "460          4.068671    0.097611    1.150560      5.752749     4.713146   \n",
       "\n",
       "     NFYA (4800)  ...  H3C2 (8358)  H3C3 (8352)  AC098582.1 (8916)  \\\n",
       "0       5.289834  ...     0.748461     0.124328           0.356144   \n",
       "1       4.542877  ...     0.000000     0.863938           0.000000   \n",
       "2       4.078097  ...     2.204767     0.000000           0.545968   \n",
       "3       5.871597  ...     1.214125     2.839960           0.286881   \n",
       "4       4.761285  ...     2.017922     1.378512           0.000000   \n",
       "..           ...  ...          ...          ...                ...   \n",
       "456     3.121015  ...     1.176323     0.275007           0.367371   \n",
       "457     4.051372  ...     0.678072     0.344828           0.411426   \n",
       "458     3.856986  ...     0.464668     0.000000           1.157044   \n",
       "459     4.440952  ...     0.298658     0.000000           0.948601   \n",
       "460     4.155425  ...     0.000000     0.000000           0.000000   \n",
       "\n",
       "     DUS4L-BCAP29 (115253422)  C8orf44-SGK3 (100533105)  ELOA3B (728929)  \\\n",
       "0                    1.761285                  0.378512         0.000000   \n",
       "1                    2.750607                  1.389567         0.111031   \n",
       "2                    2.498251                  0.250962         0.014355   \n",
       "3                    2.211012                  0.000000         0.000000   \n",
       "4                    2.555816                  0.000000         0.014355   \n",
       "..                        ...                       ...              ...   \n",
       "456                  1.367371                  0.263034         0.000000   \n",
       "457                  1.970854                  0.298658         0.000000   \n",
       "458                  2.589763                  0.000000         0.000000   \n",
       "459                  1.650765                  0.000000         0.000000   \n",
       "460                  2.498251                  0.000000         0.000000   \n",
       "\n",
       "     NPBWR1 (2831)  ELOA3D (100506888)  ELOA3 (162699)  CDR1 (1038)  \n",
       "0         0.014355            0.000000        0.226509     0.000000  \n",
       "1         0.000000            0.111031        0.111031     0.389567  \n",
       "2         0.000000            0.014355        0.014355     0.000000  \n",
       "3         0.000000            0.000000        0.000000     0.475085  \n",
       "4         0.084064            0.000000        0.014355     0.111031  \n",
       "..             ...                 ...             ...          ...  \n",
       "456       0.000000            0.000000        0.000000     0.000000  \n",
       "457       0.014355            0.000000        0.000000     0.028569  \n",
       "458       0.000000            0.150560        0.000000     0.000000  \n",
       "459       0.000000            0.000000        0.000000     0.000000  \n",
       "460       0.000000            0.000000        0.000000     0.000000  \n",
       "\n",
       "[461 rows x 19221 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1243 variables according to Pearson's r and Spearman Rank correlation were correlated to the Sensitivity\n"
     ]
    }
   ],
   "source": [
    "# Correlation testing with Filter Method\n",
    "\n",
    "\n",
    "list = []\n",
    "\n",
    "count = 0\n",
    "for var in X.columns:\n",
    "    if X[var].nunique() > 1:\n",
    "        spearmanr_corr, spearmanr_pvalue = sts.spearmanr(X[var], y)\n",
    "        if  abs(spearmanr_corr) > (0.15):\n",
    "            if (spearmanr_pvalue) < (0.05):\n",
    "                count += 1\n",
    "                list.append(var)\n",
    "\n",
    "\n",
    "count2 = 0\n",
    "for var2 in X.columns:\n",
    "    if  X[var2].nunique() > 1:\n",
    "        pearsonr_corr, pearsonr_pvalue = sts.pearsonr(X[var2], y)\n",
    "        if abs(pearsonr_corr) > (0.15):\n",
    "            if (pearsonr_pvalue) < (0.05):\n",
    "                count2 += 1\n",
    "                if var2 not in list:\n",
    "                    list.append(var2)\n",
    "print(len(list), \"variables according to Pearson's r and Spearman Rank correlation were correlated to the Sensitivity\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENPP4 (22875)</th>\n",
       "      <th>DBNDD1 (79007)</th>\n",
       "      <th>KDM1A (23028)</th>\n",
       "      <th>DHX33 (56919)</th>\n",
       "      <th>COPZ2 (51226)</th>\n",
       "      <th>CROT (54677)</th>\n",
       "      <th>ZNF195 (7748)</th>\n",
       "      <th>TMEM98 (26022)</th>\n",
       "      <th>NOS2 (4843)</th>\n",
       "      <th>LUC7L (55692)</th>\n",
       "      <th>...</th>\n",
       "      <th>PDCD6 (10016)</th>\n",
       "      <th>GPR162 (27239)</th>\n",
       "      <th>ATXN7L3B (552889)</th>\n",
       "      <th>UTP14C (9724)</th>\n",
       "      <th>MEX3A (92312)</th>\n",
       "      <th>FXYD6-FXYD2 (100533181)</th>\n",
       "      <th>TRIM34 (53840)</th>\n",
       "      <th>GFY (100507003)</th>\n",
       "      <th>XKR5 (389610)</th>\n",
       "      <th>MARCKS (4082)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.687061</td>\n",
       "      <td>3.360364</td>\n",
       "      <td>6.495056</td>\n",
       "      <td>4.987776</td>\n",
       "      <td>3.823749</td>\n",
       "      <td>4.090007</td>\n",
       "      <td>4.793376</td>\n",
       "      <td>7.221780</td>\n",
       "      <td>0.485427</td>\n",
       "      <td>6.709015</td>\n",
       "      <td>...</td>\n",
       "      <td>6.831244</td>\n",
       "      <td>3.887525</td>\n",
       "      <td>4.336997</td>\n",
       "      <td>2.361768</td>\n",
       "      <td>5.463361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.195348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622930</td>\n",
       "      <td>4.929791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.137504</td>\n",
       "      <td>2.416840</td>\n",
       "      <td>6.308521</td>\n",
       "      <td>3.481557</td>\n",
       "      <td>6.992429</td>\n",
       "      <td>5.102658</td>\n",
       "      <td>2.713696</td>\n",
       "      <td>1.726831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.439623</td>\n",
       "      <td>...</td>\n",
       "      <td>6.903400</td>\n",
       "      <td>2.060047</td>\n",
       "      <td>4.662205</td>\n",
       "      <td>0.505891</td>\n",
       "      <td>2.992768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.632268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.400538</td>\n",
       "      <td>6.917432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014355</td>\n",
       "      <td>2.599318</td>\n",
       "      <td>5.672142</td>\n",
       "      <td>4.061776</td>\n",
       "      <td>5.249445</td>\n",
       "      <td>3.392317</td>\n",
       "      <td>3.157044</td>\n",
       "      <td>6.244697</td>\n",
       "      <td>0.367371</td>\n",
       "      <td>4.635174</td>\n",
       "      <td>...</td>\n",
       "      <td>6.906289</td>\n",
       "      <td>0.516015</td>\n",
       "      <td>3.291309</td>\n",
       "      <td>0.356144</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.985500</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.014355</td>\n",
       "      <td>4.972233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014355</td>\n",
       "      <td>2.914565</td>\n",
       "      <td>6.626147</td>\n",
       "      <td>4.838952</td>\n",
       "      <td>1.989139</td>\n",
       "      <td>3.023255</td>\n",
       "      <td>5.581351</td>\n",
       "      <td>6.880686</td>\n",
       "      <td>0.748461</td>\n",
       "      <td>6.751544</td>\n",
       "      <td>...</td>\n",
       "      <td>6.349967</td>\n",
       "      <td>4.358256</td>\n",
       "      <td>6.109152</td>\n",
       "      <td>2.718088</td>\n",
       "      <td>6.373300</td>\n",
       "      <td>0.594549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.427606</td>\n",
       "      <td>6.851999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.748461</td>\n",
       "      <td>3.945795</td>\n",
       "      <td>5.942515</td>\n",
       "      <td>3.990955</td>\n",
       "      <td>5.075533</td>\n",
       "      <td>2.893362</td>\n",
       "      <td>4.037382</td>\n",
       "      <td>0.485427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.980939</td>\n",
       "      <td>...</td>\n",
       "      <td>6.466627</td>\n",
       "      <td>3.517276</td>\n",
       "      <td>4.034744</td>\n",
       "      <td>1.014355</td>\n",
       "      <td>3.397803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.389567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.715893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>2.459432</td>\n",
       "      <td>2.056584</td>\n",
       "      <td>4.728465</td>\n",
       "      <td>2.757023</td>\n",
       "      <td>1.646163</td>\n",
       "      <td>2.773996</td>\n",
       "      <td>3.760221</td>\n",
       "      <td>2.737687</td>\n",
       "      <td>2.094236</td>\n",
       "      <td>4.571677</td>\n",
       "      <td>...</td>\n",
       "      <td>6.913608</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>3.880686</td>\n",
       "      <td>1.550901</td>\n",
       "      <td>0.871844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.014355</td>\n",
       "      <td>0.097611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.499527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>0.014355</td>\n",
       "      <td>1.422233</td>\n",
       "      <td>5.619120</td>\n",
       "      <td>3.089159</td>\n",
       "      <td>5.744161</td>\n",
       "      <td>1.937344</td>\n",
       "      <td>3.229588</td>\n",
       "      <td>0.773996</td>\n",
       "      <td>0.014355</td>\n",
       "      <td>4.397803</td>\n",
       "      <td>...</td>\n",
       "      <td>6.517276</td>\n",
       "      <td>1.344828</td>\n",
       "      <td>3.722466</td>\n",
       "      <td>0.226509</td>\n",
       "      <td>1.604071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.305971</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>1.673556</td>\n",
       "      <td>6.162492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>3.282440</td>\n",
       "      <td>5.781622</td>\n",
       "      <td>6.087675</td>\n",
       "      <td>4.372256</td>\n",
       "      <td>0.214125</td>\n",
       "      <td>0.014355</td>\n",
       "      <td>4.577127</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.252098</td>\n",
       "      <td>...</td>\n",
       "      <td>6.755288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.152183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.992768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.831877</td>\n",
       "      <td>0.097611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.737687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>3.077243</td>\n",
       "      <td>1.464668</td>\n",
       "      <td>5.915999</td>\n",
       "      <td>3.555816</td>\n",
       "      <td>0.356144</td>\n",
       "      <td>3.364572</td>\n",
       "      <td>5.158660</td>\n",
       "      <td>5.630231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.081936</td>\n",
       "      <td>...</td>\n",
       "      <td>7.690627</td>\n",
       "      <td>0.226509</td>\n",
       "      <td>4.191405</td>\n",
       "      <td>3.598127</td>\n",
       "      <td>2.857981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.799087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>4.605257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>0.084064</td>\n",
       "      <td>2.518535</td>\n",
       "      <td>5.695994</td>\n",
       "      <td>4.370862</td>\n",
       "      <td>3.411426</td>\n",
       "      <td>3.468583</td>\n",
       "      <td>3.945795</td>\n",
       "      <td>3.565597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.766065</td>\n",
       "      <td>...</td>\n",
       "      <td>7.414474</td>\n",
       "      <td>0.056584</td>\n",
       "      <td>3.381283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.414136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.272770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>461 rows × 1243 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ENPP4 (22875)  DBNDD1 (79007)  KDM1A (23028)  DHX33 (56919)  \\\n",
       "0         0.687061        3.360364       6.495056       4.987776   \n",
       "1         1.137504        2.416840       6.308521       3.481557   \n",
       "2         0.014355        2.599318       5.672142       4.061776   \n",
       "3         0.014355        2.914565       6.626147       4.838952   \n",
       "4         1.748461        3.945795       5.942515       3.990955   \n",
       "..             ...             ...            ...            ...   \n",
       "456       2.459432        2.056584       4.728465       2.757023   \n",
       "457       0.014355        1.422233       5.619120       3.089159   \n",
       "458       3.282440        5.781622       6.087675       4.372256   \n",
       "459       3.077243        1.464668       5.915999       3.555816   \n",
       "460       0.084064        2.518535       5.695994       4.370862   \n",
       "\n",
       "     COPZ2 (51226)  CROT (54677)  ZNF195 (7748)  TMEM98 (26022)  NOS2 (4843)  \\\n",
       "0         3.823749      4.090007       4.793376        7.221780     0.485427   \n",
       "1         6.992429      5.102658       2.713696        1.726831     0.000000   \n",
       "2         5.249445      3.392317       3.157044        6.244697     0.367371   \n",
       "3         1.989139      3.023255       5.581351        6.880686     0.748461   \n",
       "4         5.075533      2.893362       4.037382        0.485427     0.000000   \n",
       "..             ...           ...            ...             ...          ...   \n",
       "456       1.646163      2.773996       3.760221        2.737687     2.094236   \n",
       "457       5.744161      1.937344       3.229588        0.773996     0.014355   \n",
       "458       0.214125      0.014355       4.577127        0.344828     0.000000   \n",
       "459       0.356144      3.364572       5.158660        5.630231     0.000000   \n",
       "460       3.411426      3.468583       3.945795        3.565597     0.000000   \n",
       "\n",
       "     LUC7L (55692)  ...  PDCD6 (10016)  GPR162 (27239)  ATXN7L3B (552889)  \\\n",
       "0         6.709015  ...       6.831244        3.887525           4.336997   \n",
       "1         6.439623  ...       6.903400        2.060047           4.662205   \n",
       "2         4.635174  ...       6.906289        0.516015           3.291309   \n",
       "3         6.751544  ...       6.349967        4.358256           6.109152   \n",
       "4         4.980939  ...       6.466627        3.517276           4.034744   \n",
       "..             ...  ...            ...             ...                ...   \n",
       "456       4.571677  ...       6.913608        0.028569           3.880686   \n",
       "457       4.397803  ...       6.517276        1.344828           3.722466   \n",
       "458       5.252098  ...       6.755288        0.000000           3.152183   \n",
       "459       6.081936  ...       7.690627        0.226509           4.191405   \n",
       "460       4.766065  ...       7.414474        0.056584           3.381283   \n",
       "\n",
       "     UTP14C (9724)  MEX3A (92312)  FXYD6-FXYD2 (100533181)  TRIM34 (53840)  \\\n",
       "0         2.361768       5.463361                 0.000000        1.195348   \n",
       "1         0.505891       2.992768                 0.000000        2.632268   \n",
       "2         0.356144       1.000000                 0.000000        1.985500   \n",
       "3         2.718088       6.373300                 0.594549        0.000000   \n",
       "4         1.014355       3.397803                 0.000000        1.389567   \n",
       "..             ...            ...                      ...             ...   \n",
       "456       1.550901       0.871844                 0.000000        1.014355   \n",
       "457       0.226509       1.604071                 0.000000        3.305971   \n",
       "458       0.000000       0.992768                 0.000000        0.831877   \n",
       "459       3.598127       2.857981                 0.000000        3.799087   \n",
       "460       0.000000       0.176323                 0.000000        2.414136   \n",
       "\n",
       "     GFY (100507003)  XKR5 (389610)  MARCKS (4082)  \n",
       "0           0.000000       0.622930       4.929791  \n",
       "1           0.000000       1.400538       6.917432  \n",
       "2           0.028569       0.014355       4.972233  \n",
       "3           0.000000       2.427606       6.851999  \n",
       "4           0.000000       0.000000       5.715893  \n",
       "..               ...            ...            ...  \n",
       "456         0.097611       0.000000       3.499527  \n",
       "457         0.028569       1.673556       6.162492  \n",
       "458         0.097611       0.000000       3.737687  \n",
       "459         0.000000       0.028569       4.605257  \n",
       "460         0.000000       0.000000       4.272770  \n",
       "\n",
       "[461 rows x 1243 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(X[list])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average_Test_R2</th>\n",
       "      <th>Test_R2_STD</th>\n",
       "      <th>Average_Test_RMSE</th>\n",
       "      <th>Test_RMSE_STD</th>\n",
       "      <th>Average_Train_R2</th>\n",
       "      <th>Train_R2_STD</th>\n",
       "      <th>Average_Train_RMSE</th>\n",
       "      <th>Train_RMSE_STD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear_Regression</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso_Regression</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge_Regression</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Average_Test_R2 Test_R2_STD Average_Test_RMSE Test_RMSE_STD  \\\n",
       "Linear_Regression             NaN         NaN               NaN           NaN   \n",
       "Lasso_Regression              NaN         NaN               NaN           NaN   \n",
       "Ridge_Regression              NaN         NaN               NaN           NaN   \n",
       "\n",
       "                  Average_Train_R2 Train_R2_STD Average_Train_RMSE  \\\n",
       "Linear_Regression              NaN          NaN                NaN   \n",
       "Lasso_Regression               NaN          NaN                NaN   \n",
       "Ridge_Regression               NaN          NaN                NaN   \n",
       "\n",
       "                  Train_RMSE_STD  \n",
       "Linear_Regression            NaN  \n",
       "Lasso_Regression             NaN  \n",
       "Ridge_Regression             NaN  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = pd.DataFrame(index=['Linear_Regression', 'Lasso_Regression', 'Ridge_Regression'], columns=['Average_Test_R2', 'Test_R2_STD', 'Average_Test_RMSE', 'Test_RMSE_STD', 'Average_Train_R2', 'Train_R2_STD', 'Average_Train_RMSE', 'Train_RMSE_STD'])\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fold 1\n",
      "{'pca__n_components': [40]}\n",
      "Train RMSE: 0.165, R2: 0.363\n",
      "Test RMSE: 0.179, R2: 0.160\n",
      "Working on fold 2\n",
      "{'pca__n_components': [40, 41]}\n",
      "Train RMSE: 0.165, R2: 0.363\n",
      "Test RMSE: 0.199, R2: -0.001\n",
      "Working on fold 3\n",
      "{'pca__n_components': [40, 41, 40]}\n",
      "Train RMSE: 0.167, R2: 0.325\n",
      "Test RMSE: 0.180, R2: 0.261\n",
      "Working on fold 4\n",
      "{'pca__n_components': [40, 41, 40, 35]}\n",
      "Train RMSE: 0.173, R2: 0.274\n",
      "Test RMSE: 0.177, R2: 0.308\n",
      "Working on fold 5\n",
      "{'pca__n_components': [40, 41, 40, 35, 40]}\n",
      "Train RMSE: 0.167, R2: 0.329\n",
      "Test RMSE: 0.179, R2: 0.253\n",
      "Average Train RMSE: 0.167\n",
      "Average Train R2: 0.331\n",
      "Average Test RMSE: 0.183\n",
      "Average Test R2: 0.196\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function for Linear Regression (PCA as additional Feature Selection step)\n",
    "def evaluation(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    rmse = mean_squared_error(y, y_pred, squared=False)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    return rmse, r2\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "fold = 1\n",
    "\n",
    "train_metrics = {'rmse': [], 'r2': []}\n",
    "test_metrics = {'rmse': [], 'r2': []}\n",
    "best_params = {'pca__n_components': []}\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {'pca__n_components': np.linspace(35, 42, 16, dtype=int)}\n",
    "\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    print(f\"Working on fold {fold}\")\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('pca', PCA(random_state=0)),\n",
    "                     ('linear', LinearRegression())])\n",
    "    \n",
    "    search = GridSearchCV(pipe, param_grid, n_jobs=4)\n",
    "    clf = search.fit(X_train, y_train)\n",
    "\n",
    "    best_params['pca__n_components'].append(search.best_params_['pca__n_components'])\n",
    "    print(best_params)\n",
    "\n",
    "    train_eval = evaluation(clf, X_train, y_train)\n",
    "    train_metrics['rmse'].append(train_eval[0])\n",
    "    train_metrics['r2'].append(train_eval[1])\n",
    "    \n",
    "    test_eval = evaluation(clf, X_test, y_test)\n",
    "    test_metrics['rmse'].append(test_eval[0])\n",
    "    test_metrics['r2'].append(test_eval[1])\n",
    "\n",
    "    print(f\"Train RMSE: {train_eval[0]:.3f}, R2: {train_eval[1]:.3f}\")\n",
    "    print(f\"Test RMSE: {test_eval[0]:.3f}, R2: {test_eval[1]:.3f}\")\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "\n",
    "\n",
    "# Print average metrics\n",
    "print(\"Average Train RMSE: {:.3f}\".format(np.mean(train_metrics['rmse'])))\n",
    "print(\"Average Train R2: {:.3f}\".format(np.mean(train_metrics['r2'])))\n",
    "print(\"Average Test RMSE: {:.3f}\".format(np.mean(test_metrics['rmse'])))\n",
    "print(\"Average Test R2: {:.3f}\".format(np.mean(test_metrics['r2'])))\n",
    "\n",
    "summary.loc['Linear_Regression']['Average_Train_R2'] = np.mean(train_metrics['r2'])\n",
    "summary.loc['Linear_Regression']['Average_Test_R2'] = np.mean(test_metrics['r2'])\n",
    "summary.loc['Linear_Regression']['Average_Train_RMSE'] = np.mean(train_metrics['rmse'])\n",
    "summary.loc['Linear_Regression']['Average_Test_RMSE'] = np.mean(test_metrics['rmse'])\n",
    "\n",
    "summary.loc['Linear_Regression']['Train_R2_STD'] = np.std(train_metrics['r2'])\n",
    "summary.loc['Linear_Regression']['Test_R2_STD'] = np.std(test_metrics['r2'])\n",
    "summary.loc['Linear_Regression']['Train_RMSE_STD'] = np.std(train_metrics['rmse'])\n",
    "summary.loc['Linear_Regression']['Test_RMSE_STD'] = np.std(test_metrics['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1. Performance\\n\\n'pca__n_components': np.linspace(5, 100, 10, dtype=int)\\n\\nWorking on fold 1\\n{'pca__n_components': [26]}\\nTrain RMSE: 0.171, R2: 0.318\\nTest RMSE: 0.175, R2: 0.202\\nWorking on fold 2\\n{'pca__n_components': [26, 26]}\\nTrain RMSE: 0.168, R2: 0.340\\nTest RMSE: 0.199, R2: -0.009\\nWorking on fold 3\\n{'pca__n_components': [26, 26, 47]}\\nTrain RMSE: 0.162, R2: 0.370\\nTest RMSE: 0.184, R2: 0.222\\nWorking on fold 4\\n{'pca__n_components': [26, 26, 47, 26]}\\nTrain RMSE: 0.172, R2: 0.283\\nTest RMSE: 0.180, R2: 0.285\\nWorking on fold 5\\n{'pca__n_components': [26, 26, 47, 26, 26]}\\nTrain RMSE: 0.169, R2: 0.315\\nTest RMSE: 0.173, R2: 0.304\\nAverage Train RMSE: 0.168\\nAverage Train R2: 0.325\\nAverage Test RMSE: 0.182\\nAverage Test R2: 0.201\\n\\n\\n2. Performance\\n\\n'pca__n_components': np.linspace(20, 50, 16, dtype=int)\\n\\nWorking on fold 1\\n{'pca__n_components': [32]}\\nTrain RMSE: 0.169, R2: 0.330\\nTest RMSE: 0.173, R2: 0.219\\nWorking on fold 2\\n{'pca__n_components': [32, 28]}\\nTrain RMSE: 0.166, R2: 0.350\\nTest RMSE: 0.198, R2: 0.000\\nWorking on fold 3\\n{'pca__n_components': [32, 28, 40]}\\nTrain RMSE: 0.164, R2: 0.351\\nTest RMSE: 0.186, R2: 0.211\\nWorking on fold 4\\n{'pca__n_components': [32, 28, 40, 30]}\\nTrain RMSE: 0.170, R2: 0.296\\nTest RMSE: 0.178, R2: 0.303\\nWorking on fold 5\\n{'pca__n_components': [32, 28, 40, 30, 28]}\\nTrain RMSE: 0.169, R2: 0.319\\nTest RMSE: 0.174, R2: 0.296\\nAverage Train RMSE: 0.168\\nAverage Train R2: 0.329\\nAverage Test RMSE: 0.182\\nAverage Test R2: 0.206\\n\\n\\n3. Performance\\n\\n'pca__n_components': np.linspace(30, 50, 10, dtype=int)\\n\\nWorking on fold 1\\n{'pca__n_components': [32]}\\nTrain RMSE: 0.169, R2: 0.330\\nTest RMSE: 0.173, R2: 0.219\\nWorking on fold 2\\n{'pca__n_components': [32, 30]}\\nTrain RMSE: 0.166, R2: 0.350\\nTest RMSE: 0.199, R2: -0.003\\nWorking on fold 3\\n{'pca__n_components': [32, 30, 41]}\\nTrain RMSE: 0.164, R2: 0.351\\nTest RMSE: 0.185, R2: 0.212\\nWorking on fold 4\\n{'pca__n_components': [32, 30, 41, 30]}\\nTrain RMSE: 0.170, R2: 0.296\\nTest RMSE: 0.178, R2: 0.303\\nWorking on fold 5\\n{'pca__n_components': [32, 30, 41, 30, 32]}\\nTrain RMSE: 0.168, R2: 0.327\\nTest RMSE: 0.173, R2: 0.303\\nAverage Train RMSE: 0.168\\nAverage Train R2: 0.331\\nAverage Test RMSE: 0.182\\nAverage Test R2: 0.207\\n\\n\\n4. Performance\\n\\n'pca__n_components': np.linspace(35, 50, 10, dtype=int)\\n\\nWorking on fold 1\\n{'pca__n_components': [38]}\\nTrain RMSE: 0.168, R2: 0.339\\nTest RMSE: 0.171, R2: 0.238\\nWorking on fold 2\\n{'pca__n_components': [38, 38]}\\nTrain RMSE: 0.164, R2: 0.366\\nTest RMSE: 0.195, R2: 0.032\\nWorking on fold 3\\n{'pca__n_components': [38, 38, 41]}\\nTrain RMSE: 0.164, R2: 0.351\\nTest RMSE: 0.185, R2: 0.212\\nWorking on fold 4\\n{'pca__n_components': [38, 38, 41, 35]}\\nTrain RMSE: 0.168, R2: 0.315\\nTest RMSE: 0.180, R2: 0.282\\nWorking on fold 5\\n{'pca__n_components': [38, 38, 41, 35, 35]}\\nTrain RMSE: 0.167, R2: 0.332\\nTest RMSE: 0.174, R2: 0.291\\nAverage Train RMSE: 0.166\\nAverage Train R2: 0.341\\nAverage Test RMSE: 0.181\\nAverage Test R2: 0.211\\n\\n\\n5. Performance (Best)\\n\\n'pca__n_components': np.linspace(35, 50, 16, dtype=int)\\n\\nWorking on fold 1\\n{'pca__n_components': [38]}\\nTrain RMSE: 0.168, R2: 0.339\\nTest RMSE: 0.171, R2: 0.238\\nWorking on fold 2\\n{'pca__n_components': [38, 39]}\\nTrain RMSE: 0.164, R2: 0.367\\nTest RMSE: 0.195, R2: 0.039\\nWorking on fold 3\\n{'pca__n_components': [38, 39, 41]}\\nTrain RMSE: 0.164, R2: 0.351\\nTest RMSE: 0.185, R2: 0.212\\nWorking on fold 4\\n{'pca__n_components': [38, 39, 41, 35]}\\nTrain RMSE: 0.168, R2: 0.315\\nTest RMSE: 0.180, R2: 0.282\\nWorking on fold 5\\n{'pca__n_components': [38, 39, 41, 35, 35]}\\nTrain RMSE: 0.167, R2: 0.332\\nTest RMSE: 0.174, R2: 0.291\\nAverage Train RMSE: 0.166\\nAverage Train R2: 0.341\\nAverage Test RMSE: 0.181\\nAverage Test R2: 0.212\\n\\n\\n6. Performance (Best)\\n\\n'pca__n_components': np.linspace(35, 42, 16, dtype=int)\\n\\nWorking on fold 1\\n{'pca__n_components': [38]}\\nTrain RMSE: 0.168, R2: 0.339\\nTest RMSE: 0.171, R2: 0.238\\nWorking on fold 2\\n{'pca__n_components': [38, 39]}\\nTrain RMSE: 0.164, R2: 0.367\\nTest RMSE: 0.195, R2: 0.039\\nWorking on fold 3\\n{'pca__n_components': [38, 39, 41]}\\nTrain RMSE: 0.164, R2: 0.351\\nTest RMSE: 0.185, R2: 0.212\\nWorking on fold 4\\n{'pca__n_components': [38, 39, 41, 35]}\\nTrain RMSE: 0.168, R2: 0.315\\nTest RMSE: 0.180, R2: 0.282\\nWorking on fold 5\\n{'pca__n_components': [38, 39, 41, 35, 35]}\\nTrain RMSE: 0.167, R2: 0.332\\nTest RMSE: 0.174, R2: 0.291\\nAverage Train RMSE: 0.166\\nAverage Train R2: 0.341\\nAverage Test RMSE: 0.181\\nAverage Test R2: 0.212\\n\\n\""
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Performance\n",
    "\n",
    "'pca__n_components': np.linspace(5, 100, 10, dtype=int)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [26]}\n",
    "Train RMSE: 0.171, R2: 0.318\n",
    "Test RMSE: 0.175, R2: 0.202\n",
    "Working on fold 2\n",
    "{'pca__n_components': [26, 26]}\n",
    "Train RMSE: 0.168, R2: 0.340\n",
    "Test RMSE: 0.199, R2: -0.009\n",
    "Working on fold 3\n",
    "{'pca__n_components': [26, 26, 47]}\n",
    "Train RMSE: 0.162, R2: 0.370\n",
    "Test RMSE: 0.184, R2: 0.222\n",
    "Working on fold 4\n",
    "{'pca__n_components': [26, 26, 47, 26]}\n",
    "Train RMSE: 0.172, R2: 0.283\n",
    "Test RMSE: 0.180, R2: 0.285\n",
    "Working on fold 5\n",
    "{'pca__n_components': [26, 26, 47, 26, 26]}\n",
    "Train RMSE: 0.169, R2: 0.315\n",
    "Test RMSE: 0.173, R2: 0.304\n",
    "Average Train RMSE: 0.168\n",
    "Average Train R2: 0.325\n",
    "Average Test RMSE: 0.182\n",
    "Average Test R2: 0.201\n",
    "\n",
    "\n",
    "2. Performance\n",
    "\n",
    "'pca__n_components': np.linspace(20, 50, 16, dtype=int)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [32]}\n",
    "Train RMSE: 0.169, R2: 0.330\n",
    "Test RMSE: 0.173, R2: 0.219\n",
    "Working on fold 2\n",
    "{'pca__n_components': [32, 28]}\n",
    "Train RMSE: 0.166, R2: 0.350\n",
    "Test RMSE: 0.198, R2: 0.000\n",
    "Working on fold 3\n",
    "{'pca__n_components': [32, 28, 40]}\n",
    "Train RMSE: 0.164, R2: 0.351\n",
    "Test RMSE: 0.186, R2: 0.211\n",
    "Working on fold 4\n",
    "{'pca__n_components': [32, 28, 40, 30]}\n",
    "Train RMSE: 0.170, R2: 0.296\n",
    "Test RMSE: 0.178, R2: 0.303\n",
    "Working on fold 5\n",
    "{'pca__n_components': [32, 28, 40, 30, 28]}\n",
    "Train RMSE: 0.169, R2: 0.319\n",
    "Test RMSE: 0.174, R2: 0.296\n",
    "Average Train RMSE: 0.168\n",
    "Average Train R2: 0.329\n",
    "Average Test RMSE: 0.182\n",
    "Average Test R2: 0.206\n",
    "\n",
    "\n",
    "3. Performance\n",
    "\n",
    "'pca__n_components': np.linspace(30, 50, 10, dtype=int)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [32]}\n",
    "Train RMSE: 0.169, R2: 0.330\n",
    "Test RMSE: 0.173, R2: 0.219\n",
    "Working on fold 2\n",
    "{'pca__n_components': [32, 30]}\n",
    "Train RMSE: 0.166, R2: 0.350\n",
    "Test RMSE: 0.199, R2: -0.003\n",
    "Working on fold 3\n",
    "{'pca__n_components': [32, 30, 41]}\n",
    "Train RMSE: 0.164, R2: 0.351\n",
    "Test RMSE: 0.185, R2: 0.212\n",
    "Working on fold 4\n",
    "{'pca__n_components': [32, 30, 41, 30]}\n",
    "Train RMSE: 0.170, R2: 0.296\n",
    "Test RMSE: 0.178, R2: 0.303\n",
    "Working on fold 5\n",
    "{'pca__n_components': [32, 30, 41, 30, 32]}\n",
    "Train RMSE: 0.168, R2: 0.327\n",
    "Test RMSE: 0.173, R2: 0.303\n",
    "Average Train RMSE: 0.168\n",
    "Average Train R2: 0.331\n",
    "Average Test RMSE: 0.182\n",
    "Average Test R2: 0.207\n",
    "\n",
    "\n",
    "4. Performance\n",
    "\n",
    "'pca__n_components': np.linspace(35, 50, 10, dtype=int)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [38]}\n",
    "Train RMSE: 0.168, R2: 0.339\n",
    "Test RMSE: 0.171, R2: 0.238\n",
    "Working on fold 2\n",
    "{'pca__n_components': [38, 38]}\n",
    "Train RMSE: 0.164, R2: 0.366\n",
    "Test RMSE: 0.195, R2: 0.032\n",
    "Working on fold 3\n",
    "{'pca__n_components': [38, 38, 41]}\n",
    "Train RMSE: 0.164, R2: 0.351\n",
    "Test RMSE: 0.185, R2: 0.212\n",
    "Working on fold 4\n",
    "{'pca__n_components': [38, 38, 41, 35]}\n",
    "Train RMSE: 0.168, R2: 0.315\n",
    "Test RMSE: 0.180, R2: 0.282\n",
    "Working on fold 5\n",
    "{'pca__n_components': [38, 38, 41, 35, 35]}\n",
    "Train RMSE: 0.167, R2: 0.332\n",
    "Test RMSE: 0.174, R2: 0.291\n",
    "Average Train RMSE: 0.166\n",
    "Average Train R2: 0.341\n",
    "Average Test RMSE: 0.181\n",
    "Average Test R2: 0.211\n",
    "\n",
    "\n",
    "5. Performance (Best)\n",
    "\n",
    "'pca__n_components': np.linspace(35, 50, 16, dtype=int)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [38]}\n",
    "Train RMSE: 0.168, R2: 0.339\n",
    "Test RMSE: 0.171, R2: 0.238\n",
    "Working on fold 2\n",
    "{'pca__n_components': [38, 39]}\n",
    "Train RMSE: 0.164, R2: 0.367\n",
    "Test RMSE: 0.195, R2: 0.039\n",
    "Working on fold 3\n",
    "{'pca__n_components': [38, 39, 41]}\n",
    "Train RMSE: 0.164, R2: 0.351\n",
    "Test RMSE: 0.185, R2: 0.212\n",
    "Working on fold 4\n",
    "{'pca__n_components': [38, 39, 41, 35]}\n",
    "Train RMSE: 0.168, R2: 0.315\n",
    "Test RMSE: 0.180, R2: 0.282\n",
    "Working on fold 5\n",
    "{'pca__n_components': [38, 39, 41, 35, 35]}\n",
    "Train RMSE: 0.167, R2: 0.332\n",
    "Test RMSE: 0.174, R2: 0.291\n",
    "Average Train RMSE: 0.166\n",
    "Average Train R2: 0.341\n",
    "Average Test RMSE: 0.181\n",
    "Average Test R2: 0.212\n",
    "\n",
    "\n",
    "6. Performance (Best)\n",
    "\n",
    "'pca__n_components': np.linspace(35, 42, 16, dtype=int)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [38]}\n",
    "Train RMSE: 0.168, R2: 0.339\n",
    "Test RMSE: 0.171, R2: 0.238\n",
    "Working on fold 2\n",
    "{'pca__n_components': [38, 39]}\n",
    "Train RMSE: 0.164, R2: 0.367\n",
    "Test RMSE: 0.195, R2: 0.039\n",
    "Working on fold 3\n",
    "{'pca__n_components': [38, 39, 41]}\n",
    "Train RMSE: 0.164, R2: 0.351\n",
    "Test RMSE: 0.185, R2: 0.212\n",
    "Working on fold 4\n",
    "{'pca__n_components': [38, 39, 41, 35]}\n",
    "Train RMSE: 0.168, R2: 0.315\n",
    "Test RMSE: 0.180, R2: 0.282\n",
    "Working on fold 5\n",
    "{'pca__n_components': [38, 39, 41, 35, 35]}\n",
    "Train RMSE: 0.167, R2: 0.332\n",
    "Test RMSE: 0.174, R2: 0.291\n",
    "Average Train RMSE: 0.166\n",
    "Average Train R2: 0.341\n",
    "Average Test RMSE: 0.181\n",
    "Average Test R2: 0.212\n",
    "\n",
    "\n",
    "Best Performance with correlation threshold set at 0.15\n",
    "\n",
    "'pca__n_components': np.linspace(35, 42, 16, dtype=int)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [40]}\n",
    "Train RMSE: 0.165, R2: 0.363\n",
    "Test RMSE: 0.179, R2: 0.160\n",
    "Working on fold 2\n",
    "{'pca__n_components': [40, 41]}\n",
    "Train RMSE: 0.165, R2: 0.363\n",
    "Test RMSE: 0.199, R2: -0.001\n",
    "Working on fold 3\n",
    "{'pca__n_components': [40, 41, 40]}\n",
    "Train RMSE: 0.167, R2: 0.325\n",
    "Test RMSE: 0.180, R2: 0.261\n",
    "Working on fold 4\n",
    "{'pca__n_components': [40, 41, 40, 35]}\n",
    "Train RMSE: 0.173, R2: 0.274\n",
    "Test RMSE: 0.177, R2: 0.308\n",
    "Working on fold 5\n",
    "{'pca__n_components': [40, 41, 40, 35, 40]}\n",
    "Train RMSE: 0.167, R2: 0.329\n",
    "Test RMSE: 0.179, R2: 0.253\n",
    "Average Train RMSE: 0.167\n",
    "Average Train R2: 0.331\n",
    "Average Test RMSE: 0.183\n",
    "Average Test R2: 0.196\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fold 1\n",
      "{'pca__n_components': [65], 'lasso__alpha': [0.009]}\n",
      "Train RMSE: 0.160, R2: 0.401\n",
      "Test RMSE: 0.172, R2: 0.224\n",
      "Working on fold 2\n",
      "{'pca__n_components': [65, 75], 'lasso__alpha': [0.009, 0.009]}\n",
      "Train RMSE: 0.156, R2: 0.430\n",
      "Test RMSE: 0.196, R2: 0.029\n",
      "Working on fold 3\n",
      "{'pca__n_components': [65, 75, 54], 'lasso__alpha': [0.009, 0.009, 0.009]}\n",
      "Train RMSE: 0.163, R2: 0.358\n",
      "Test RMSE: 0.182, R2: 0.243\n",
      "Working on fold 4\n",
      "{'pca__n_components': [65, 75, 54, 80], 'lasso__alpha': [0.009, 0.009, 0.009, 0.009]}\n",
      "Train RMSE: 0.157, R2: 0.402\n",
      "Test RMSE: 0.172, R2: 0.348\n",
      "Working on fold 5\n",
      "{'pca__n_components': [65, 75, 54, 80, 65], 'lasso__alpha': [0.009, 0.009, 0.009, 0.009, 0.009]}\n",
      "Train RMSE: 0.161, R2: 0.376\n",
      "Test RMSE: 0.176, R2: 0.275\n",
      "Average Train RMSE: 0.160\n",
      "Average Train R2: 0.393\n",
      "Average Test RMSE: 0.180\n",
      "Average Test R2: 0.224\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function for Lasso Regression (PCA as additional Feature Selection step)\n",
    "def evaluation(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    rmse = mean_squared_error(y, y_pred, squared=False)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    return rmse, r2\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "fold = 1\n",
    "\n",
    "train_metrics = {'rmse': [], 'r2': []}\n",
    "test_metrics = {'rmse': [], 'r2': []}\n",
    "best_params = {'pca__n_components': [], 'lasso__alpha': []}\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {'pca__n_components': np.linspace(40, 80, 20, dtype=int),\n",
    "              'lasso__alpha': np.linspace(0.008, 0.009, 20)}\n",
    "\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    print(f\"Working on fold {fold}\")\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('pca', PCA(random_state=0)),\n",
    "                     ('lasso', Lasso())])\n",
    "    \n",
    "    search = GridSearchCV(pipe, param_grid, n_jobs=4)\n",
    "    clf = search.fit(X_train, y_train)\n",
    "\n",
    "    best_params['pca__n_components'].append(search.best_params_['pca__n_components'])\n",
    "    best_params['lasso__alpha'].append(search.best_params_['lasso__alpha'])\n",
    "    print(best_params)\n",
    "\n",
    "    train_eval = evaluation(clf, X_train, y_train)\n",
    "    train_metrics['rmse'].append(train_eval[0])\n",
    "    train_metrics['r2'].append(train_eval[1])\n",
    "    \n",
    "    test_eval = evaluation(clf, X_test, y_test)\n",
    "    test_metrics['rmse'].append(test_eval[0])\n",
    "    test_metrics['r2'].append(test_eval[1])\n",
    "\n",
    "    print(f\"Train RMSE: {train_eval[0]:.3f}, R2: {train_eval[1]:.3f}\")\n",
    "    print(f\"Test RMSE: {test_eval[0]:.3f}, R2: {test_eval[1]:.3f}\")\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "\n",
    "\n",
    "# Print average metrics\n",
    "print(\"Average Train RMSE: {:.3f}\".format(np.mean(train_metrics['rmse'])))\n",
    "print(\"Average Train R2: {:.3f}\".format(np.mean(train_metrics['r2'])))\n",
    "print(\"Average Test RMSE: {:.3f}\".format(np.mean(test_metrics['rmse'])))\n",
    "print(\"Average Test R2: {:.3f}\".format(np.mean(test_metrics['r2'])))\n",
    "\n",
    "summary.loc['Lasso_Regression']['Average_Train_R2'] = np.mean(train_metrics['r2'])\n",
    "summary.loc['Lasso_Regression']['Average_Test_R2'] = np.mean(test_metrics['r2'])\n",
    "summary.loc['Lasso_Regression']['Average_Train_RMSE'] = np.mean(train_metrics['rmse'])\n",
    "summary.loc['Lasso_Regression']['Average_Test_RMSE'] = np.mean(test_metrics['rmse'])\n",
    "\n",
    "summary.loc['Lasso_Regression']['Train_R2_STD'] = np.std(train_metrics['r2'])\n",
    "summary.loc['Lasso_Regression']['Test_R2_STD'] = np.std(test_metrics['r2'])\n",
    "summary.loc['Lasso_Regression']['Train_RMSE_STD'] = np.std(train_metrics['rmse'])\n",
    "summary.loc['Lasso_Regression']['Test_RMSE_STD'] = np.std(test_metrics['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1. Performance\\n\\n'pca__n_components': np.linspace(5, 100, 10, dtype=int)\\n'lasso__alpha': np.logspace(-4, 1, 50)\\n\\nWorking on fold 1\\n{'pca__n_components': [47], 'lasso__alpha': [0.008685113737513529]}\\nTrain RMSE: 0.170, R2: 0.323\\nTest RMSE: 0.170, R2: 0.243\\nWorking on fold 2\\n{'pca__n_components': [47, 68], 'lasso__alpha': [0.008685113737513529, 0.013894954943731374]}\\nTrain RMSE: 0.170, R2: 0.320\\nTest RMSE: 0.196, R2: 0.024\\nWorking on fold 3\\n{'pca__n_components': [47, 68, 47], 'lasso__alpha': [0.008685113737513529, 0.013894954943731374, 0.00339322177189533]}\\nTrain RMSE: 0.163, R2: 0.362\\nTest RMSE: 0.182, R2: 0.241\\nWorking on fold 4\\n{'pca__n_components': [47, 68, 47, 78], 'lasso__alpha': [0.008685113737513529, 0.013894954943731374, 0.00339322177189533, 0.008685113737513529]}\\nTrain RMSE: 0.163, R2: 0.353\\nTest RMSE: 0.176, R2: 0.314\\nWorking on fold 5\\n{'pca__n_components': [47, 68, 47, 78, 68], 'lasso__alpha': [0.008685113737513529, 0.013894954943731374, 0.00339322177189533, 0.008685113737513529, 0.008685113737513529]}\\nTrain RMSE: 0.163, R2: 0.366\\nTest RMSE: 0.179, R2: 0.253\\nAverage Train RMSE: 0.166\\nAverage Train R2: 0.345\\nAverage Test RMSE: 0.181\\nAverage Test R2: 0.215\\n\\n\\n2. Performance (Best)\\n\\n'pca__n_components': np.linspace(40, 80, 16, dtype=int)\\n'lasso__alpha': np.logspace(-3, -1, 50)\\n\\nWorking on fold 1\\n{'pca__n_components': [48], 'lasso__alpha': [0.00868511373751352]}\\nTrain RMSE: 0.170, R2: 0.324\\nTest RMSE: 0.170, R2: 0.246\\nWorking on fold 2\\n{'pca__n_components': [48, 64], 'lasso__alpha': [0.00868511373751352, 0.010481131341546858]}\\nTrain RMSE: 0.167, R2: 0.349\\nTest RMSE: 0.194, R2: 0.043\\nWorking on fold 3\\n{'pca__n_components': [48, 64, 48], 'lasso__alpha': [0.00868511373751352, 0.010481131341546858, 0.004094915062380423]}\\nTrain RMSE: 0.163, R2: 0.359\\nTest RMSE: 0.182, R2: 0.243\\nWorking on fold 4\\n{'pca__n_components': [48, 64, 48, 77], 'lasso__alpha': [0.00868511373751352, 0.010481131341546858, 0.004094915062380423, 0.0079060432109077]}\\nTrain RMSE: 0.162, R2: 0.363\\nTest RMSE: 0.176, R2: 0.313\\nWorking on fold 5\\n{'pca__n_components': [48, 64, 48, 77, 69], 'lasso__alpha': [0.00868511373751352, 0.010481131341546858, 0.004094915062380423, 0.0079060432109077, 0.009540954763499945]}\\nTrain RMSE: 0.164, R2: 0.357\\nTest RMSE: 0.179, R2: 0.254\\nAverage Train RMSE: 0.165\\nAverage Train R2: 0.350\\nAverage Test RMSE: 0.180\\nAverage Test R2: 0.220\\n\\n\\n3. Performance\\n\\n'pca__n_components': np.linspace(47, 90, 16, dtype=int)\\n'lasso__alpha': np.logspace(-3, -2, 50)\\n\\nWorking on fold 1\\n{'pca__n_components': [47], 'lasso__alpha': [0.008286427728546842]}\\nTrain RMSE: 0.170, R2: 0.326\\nTest RMSE: 0.170, R2: 0.244\\nWorking on fold 2\\n{'pca__n_components': [47, 64], 'lasso__alpha': [0.008286427728546842, 0.01]}\\nTrain RMSE: 0.166, R2: 0.354\\nTest RMSE: 0.194, R2: 0.045\\nWorking on fold 3\\n{'pca__n_components': [47, 64, 49], 'lasso__alpha': [0.008286427728546842, 0.01, 0.003906939937054617]}\\nTrain RMSE: 0.162, R2: 0.368\\nTest RMSE: 0.184, R2: 0.227\\nWorking on fold 4\\n{'pca__n_components': [47, 64, 49, 78], 'lasso__alpha': [0.008286427728546842, 0.01, 0.003906939937054617, 0.0079060432109077]}\\nTrain RMSE: 0.162, R2: 0.363\\nTest RMSE: 0.176, R2: 0.313\\nWorking on fold 5\\n{'pca__n_components': [47, 64, 49, 78, 67], 'lasso__alpha': [0.008286427728546842, 0.01, 0.003906939937054617, 0.0079060432109077, 0.009540954763499945]}\\nTrain RMSE: 0.164, R2: 0.357\\nTest RMSE: 0.179, R2: 0.254\\nAverage Train RMSE: 0.165\\nAverage Train R2: 0.354\\nAverage Test RMSE: 0.181\\nAverage Test R2: 0.217\\n\\n\\n4. Performance\\n\\nnp.linspace(50, 80, 16, dtype=int)\\n'lasso__alpha': np.linspace(0.007, 0.009, 50)\\n\\nWorking on fold 1\\n{'pca__n_components': [50], 'lasso__alpha': [0.009]}\\nTrain RMSE: 0.170, R2: 0.322\\nTest RMSE: 0.170, R2: 0.245\\nWorking on fold 2\\n{'pca__n_components': [50, 64], 'lasso__alpha': [0.009, 0.009]}\\nTrain RMSE: 0.165, R2: 0.365\\nTest RMSE: 0.194, R2: 0.048\\nWorking on fold 3\\n{'pca__n_components': [50, 64, 50], 'lasso__alpha': [0.009, 0.009, 0.007]}\\nTrain RMSE: 0.165, R2: 0.347\\nTest RMSE: 0.182, R2: 0.237\\nWorking on fold 4\\n{'pca__n_components': [50, 64, 50, 78], 'lasso__alpha': [0.009, 0.009, 0.007, 0.008020408163265305]}\\nTrain RMSE: 0.162, R2: 0.361\\nTest RMSE: 0.176, R2: 0.313\\nWorking on fold 5\\n{'pca__n_components': [50, 64, 50, 78, 68], 'lasso__alpha': [0.009, 0.009, 0.007, 0.008020408163265305, 0.009]}\\nTrain RMSE: 0.163, R2: 0.363\\nTest RMSE: 0.179, R2: 0.254\\nAverage Train RMSE: 0.165\\nAverage Train R2: 0.352\\nAverage Test RMSE: 0.180\\nAverage Test R2: 0.219\\n\\n\\n5. Performance\\n\\n'pca__n_components': np.linspace(65, 80, 16, dtype=int)\\n'lasso__alpha': np.logspace(-4, -3, 50)\\n\\nWorking on fold 1\\n{'pca__n_components': [73], 'lasso__alpha': [0.001]}\\nTrain RMSE: 0.159, R2: 0.408\\nTest RMSE: 0.167, R2: 0.272\\nWorking on fold 2\\n{'pca__n_components': [73, 65], 'lasso__alpha': [0.001, 0.001]}\\nTrain RMSE: 0.156, R2: 0.431\\nTest RMSE: 0.196, R2: 0.025\\nWorking on fold 3\\n{'pca__n_components': [73, 65, 65], 'lasso__alpha': [0.001, 0.001, 0.001]}\\nTrain RMSE: 0.159, R2: 0.395\\nTest RMSE: 0.181, R2: 0.252\\nWorking on fold 4\\n{'pca__n_components': [73, 65, 65, 65], 'lasso__alpha': [0.001, 0.001, 0.001, 0.001]}\\nTrain RMSE: 0.157, R2: 0.400\\nTest RMSE: 0.185, R2: 0.240\\nWorking on fold 5\\n{'pca__n_components': [73, 65, 65, 65, 65], 'lasso__alpha': [0.001, 0.001, 0.001, 0.001, 0.001]}\\nTrain RMSE: 0.155, R2: 0.425\\nTest RMSE: 0.188, R2: 0.176\\nAverage Train RMSE: 0.157\\nAverage Train R2: 0.412\\nAverage Test RMSE: 0.183\\nAverage Test R2: 0.193\\n\\n\\n6. Performance (Best)\\n\\n'pca__n_components': np.linspace(40, 80, 20, dtype=int)\\n'lasso__alpha': np.linspace(0.004, 0.01, 20)\\n\\nWorking on fold 1\\n{'pca__n_components': [48], 'lasso__alpha': [0.008736842105263157]}\\nTrain RMSE: 0.170, R2: 0.324\\nTest RMSE: 0.170, R2: 0.246\\nWorking on fold 2\\n{'pca__n_components': [48, 65], 'lasso__alpha': [0.008736842105263157, 0.01]}\\nTrain RMSE: 0.166, R2: 0.354\\nTest RMSE: 0.194, R2: 0.045\\nWorking on fold 3\\n{'pca__n_components': [48, 65, 48], 'lasso__alpha': [0.008736842105263157, 0.01, 0.004]}\\nTrain RMSE: 0.163, R2: 0.360\\nTest RMSE: 0.182, R2: 0.242\\nWorking on fold 4\\n{'pca__n_components': [48, 65, 48, 77], 'lasso__alpha': [0.008736842105263157, 0.01, 0.004, 0.008105263157894737]}\\nTrain RMSE: 0.162, R2: 0.360\\nTest RMSE: 0.176, R2: 0.313\\nWorking on fold 5\\n{'pca__n_components': [48, 65, 48, 77, 67], 'lasso__alpha': [0.008736842105263157, 0.01, 0.004, 0.008105263157894737, 0.009368421052631578]}\\nTrain RMSE: 0.164, R2: 0.359\\nTest RMSE: 0.179, R2: 0.254\\nAverage Train RMSE: 0.165\\nAverage Train R2: 0.351\\nAverage Test RMSE: 0.180\\nAverage Test R2: 0.220\\n\\n\\n7. Performance (Best)\\n\\n'pca__n_components': np.linspace(40, 80, 20, dtype=int),\\n'lasso__alpha': np.linspace(0.008, 0.009, 20)\\n\\nWorking on fold 1\\n{'pca__n_components': [48], 'lasso__alpha': [0.008684210526315789]}\\nTrain RMSE: 0.170, R2: 0.324\\nTest RMSE: 0.170, R2: 0.246\\nWorking on fold 2\\n{'pca__n_components': [48, 65], 'lasso__alpha': [0.008684210526315789, 0.009]}\\nTrain RMSE: 0.165, R2: 0.365\\nTest RMSE: 0.194, R2: 0.048\\nWorking on fold 3\\n{'pca__n_components': [48, 65, 54], 'lasso__alpha': [0.008684210526315789, 0.009, 0.008]}\\nTrain RMSE: 0.165, R2: 0.342\\nTest RMSE: 0.182, R2: 0.241\\nWorking on fold 4\\n{'pca__n_components': [48, 65, 54, 77], 'lasso__alpha': [0.008684210526315789, 0.009, 0.008, 0.008]}\\nTrain RMSE: 0.162, R2: 0.362\\nTest RMSE: 0.176, R2: 0.313\\nWorking on fold 5\\n{'pca__n_components': [48, 65, 54, 77, 67], 'lasso__alpha': [0.008684210526315789, 0.009, 0.008, 0.008, 0.009]}\\nTrain RMSE: 0.163, R2: 0.363\\nTest RMSE: 0.179, R2: 0.254\\nAverage Train RMSE: 0.165\\nAverage Train R2: 0.351\\nAverage Test RMSE: 0.180\\nAverage Test R2: 0.220\\n\""
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Performance\n",
    "\n",
    "'pca__n_components': np.linspace(5, 100, 10, dtype=int)\n",
    "'lasso__alpha': np.logspace(-4, 1, 50)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [47], 'lasso__alpha': [0.008685113737513529]}\n",
    "Train RMSE: 0.170, R2: 0.323\n",
    "Test RMSE: 0.170, R2: 0.243\n",
    "Working on fold 2\n",
    "{'pca__n_components': [47, 68], 'lasso__alpha': [0.008685113737513529, 0.013894954943731374]}\n",
    "Train RMSE: 0.170, R2: 0.320\n",
    "Test RMSE: 0.196, R2: 0.024\n",
    "Working on fold 3\n",
    "{'pca__n_components': [47, 68, 47], 'lasso__alpha': [0.008685113737513529, 0.013894954943731374, 0.00339322177189533]}\n",
    "Train RMSE: 0.163, R2: 0.362\n",
    "Test RMSE: 0.182, R2: 0.241\n",
    "Working on fold 4\n",
    "{'pca__n_components': [47, 68, 47, 78], 'lasso__alpha': [0.008685113737513529, 0.013894954943731374, 0.00339322177189533, 0.008685113737513529]}\n",
    "Train RMSE: 0.163, R2: 0.353\n",
    "Test RMSE: 0.176, R2: 0.314\n",
    "Working on fold 5\n",
    "{'pca__n_components': [47, 68, 47, 78, 68], 'lasso__alpha': [0.008685113737513529, 0.013894954943731374, 0.00339322177189533, 0.008685113737513529, 0.008685113737513529]}\n",
    "Train RMSE: 0.163, R2: 0.366\n",
    "Test RMSE: 0.179, R2: 0.253\n",
    "Average Train RMSE: 0.166\n",
    "Average Train R2: 0.345\n",
    "Average Test RMSE: 0.181\n",
    "Average Test R2: 0.215\n",
    "\n",
    "\n",
    "2. Performance (Best)\n",
    "\n",
    "'pca__n_components': np.linspace(40, 80, 16, dtype=int)\n",
    "'lasso__alpha': np.logspace(-3, -1, 50)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [48], 'lasso__alpha': [0.00868511373751352]}\n",
    "Train RMSE: 0.170, R2: 0.324\n",
    "Test RMSE: 0.170, R2: 0.246\n",
    "Working on fold 2\n",
    "{'pca__n_components': [48, 64], 'lasso__alpha': [0.00868511373751352, 0.010481131341546858]}\n",
    "Train RMSE: 0.167, R2: 0.349\n",
    "Test RMSE: 0.194, R2: 0.043\n",
    "Working on fold 3\n",
    "{'pca__n_components': [48, 64, 48], 'lasso__alpha': [0.00868511373751352, 0.010481131341546858, 0.004094915062380423]}\n",
    "Train RMSE: 0.163, R2: 0.359\n",
    "Test RMSE: 0.182, R2: 0.243\n",
    "Working on fold 4\n",
    "{'pca__n_components': [48, 64, 48, 77], 'lasso__alpha': [0.00868511373751352, 0.010481131341546858, 0.004094915062380423, 0.0079060432109077]}\n",
    "Train RMSE: 0.162, R2: 0.363\n",
    "Test RMSE: 0.176, R2: 0.313\n",
    "Working on fold 5\n",
    "{'pca__n_components': [48, 64, 48, 77, 69], 'lasso__alpha': [0.00868511373751352, 0.010481131341546858, 0.004094915062380423, 0.0079060432109077, 0.009540954763499945]}\n",
    "Train RMSE: 0.164, R2: 0.357\n",
    "Test RMSE: 0.179, R2: 0.254\n",
    "Average Train RMSE: 0.165\n",
    "Average Train R2: 0.350\n",
    "Average Test RMSE: 0.180\n",
    "Average Test R2: 0.220\n",
    "\n",
    "\n",
    "3. Performance\n",
    "\n",
    "'pca__n_components': np.linspace(47, 90, 16, dtype=int)\n",
    "'lasso__alpha': np.logspace(-3, -2, 50)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [47], 'lasso__alpha': [0.008286427728546842]}\n",
    "Train RMSE: 0.170, R2: 0.326\n",
    "Test RMSE: 0.170, R2: 0.244\n",
    "Working on fold 2\n",
    "{'pca__n_components': [47, 64], 'lasso__alpha': [0.008286427728546842, 0.01]}\n",
    "Train RMSE: 0.166, R2: 0.354\n",
    "Test RMSE: 0.194, R2: 0.045\n",
    "Working on fold 3\n",
    "{'pca__n_components': [47, 64, 49], 'lasso__alpha': [0.008286427728546842, 0.01, 0.003906939937054617]}\n",
    "Train RMSE: 0.162, R2: 0.368\n",
    "Test RMSE: 0.184, R2: 0.227\n",
    "Working on fold 4\n",
    "{'pca__n_components': [47, 64, 49, 78], 'lasso__alpha': [0.008286427728546842, 0.01, 0.003906939937054617, 0.0079060432109077]}\n",
    "Train RMSE: 0.162, R2: 0.363\n",
    "Test RMSE: 0.176, R2: 0.313\n",
    "Working on fold 5\n",
    "{'pca__n_components': [47, 64, 49, 78, 67], 'lasso__alpha': [0.008286427728546842, 0.01, 0.003906939937054617, 0.0079060432109077, 0.009540954763499945]}\n",
    "Train RMSE: 0.164, R2: 0.357\n",
    "Test RMSE: 0.179, R2: 0.254\n",
    "Average Train RMSE: 0.165\n",
    "Average Train R2: 0.354\n",
    "Average Test RMSE: 0.181\n",
    "Average Test R2: 0.217\n",
    "\n",
    "\n",
    "4. Performance\n",
    "\n",
    "np.linspace(50, 80, 16, dtype=int)\n",
    "'lasso__alpha': np.linspace(0.007, 0.009, 50)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [50], 'lasso__alpha': [0.009]}\n",
    "Train RMSE: 0.170, R2: 0.322\n",
    "Test RMSE: 0.170, R2: 0.245\n",
    "Working on fold 2\n",
    "{'pca__n_components': [50, 64], 'lasso__alpha': [0.009, 0.009]}\n",
    "Train RMSE: 0.165, R2: 0.365\n",
    "Test RMSE: 0.194, R2: 0.048\n",
    "Working on fold 3\n",
    "{'pca__n_components': [50, 64, 50], 'lasso__alpha': [0.009, 0.009, 0.007]}\n",
    "Train RMSE: 0.165, R2: 0.347\n",
    "Test RMSE: 0.182, R2: 0.237\n",
    "Working on fold 4\n",
    "{'pca__n_components': [50, 64, 50, 78], 'lasso__alpha': [0.009, 0.009, 0.007, 0.008020408163265305]}\n",
    "Train RMSE: 0.162, R2: 0.361\n",
    "Test RMSE: 0.176, R2: 0.313\n",
    "Working on fold 5\n",
    "{'pca__n_components': [50, 64, 50, 78, 68], 'lasso__alpha': [0.009, 0.009, 0.007, 0.008020408163265305, 0.009]}\n",
    "Train RMSE: 0.163, R2: 0.363\n",
    "Test RMSE: 0.179, R2: 0.254\n",
    "Average Train RMSE: 0.165\n",
    "Average Train R2: 0.352\n",
    "Average Test RMSE: 0.180\n",
    "Average Test R2: 0.219\n",
    "\n",
    "\n",
    "5. Performance\n",
    "\n",
    "'pca__n_components': np.linspace(65, 80, 16, dtype=int)\n",
    "'lasso__alpha': np.logspace(-4, -3, 50)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [73], 'lasso__alpha': [0.001]}\n",
    "Train RMSE: 0.159, R2: 0.408\n",
    "Test RMSE: 0.167, R2: 0.272\n",
    "Working on fold 2\n",
    "{'pca__n_components': [73, 65], 'lasso__alpha': [0.001, 0.001]}\n",
    "Train RMSE: 0.156, R2: 0.431\n",
    "Test RMSE: 0.196, R2: 0.025\n",
    "Working on fold 3\n",
    "{'pca__n_components': [73, 65, 65], 'lasso__alpha': [0.001, 0.001, 0.001]}\n",
    "Train RMSE: 0.159, R2: 0.395\n",
    "Test RMSE: 0.181, R2: 0.252\n",
    "Working on fold 4\n",
    "{'pca__n_components': [73, 65, 65, 65], 'lasso__alpha': [0.001, 0.001, 0.001, 0.001]}\n",
    "Train RMSE: 0.157, R2: 0.400\n",
    "Test RMSE: 0.185, R2: 0.240\n",
    "Working on fold 5\n",
    "{'pca__n_components': [73, 65, 65, 65, 65], 'lasso__alpha': [0.001, 0.001, 0.001, 0.001, 0.001]}\n",
    "Train RMSE: 0.155, R2: 0.425\n",
    "Test RMSE: 0.188, R2: 0.176\n",
    "Average Train RMSE: 0.157\n",
    "Average Train R2: 0.412\n",
    "Average Test RMSE: 0.183\n",
    "Average Test R2: 0.193\n",
    "\n",
    "\n",
    "6. Performance (Best)\n",
    "\n",
    "'pca__n_components': np.linspace(40, 80, 20, dtype=int)\n",
    "'lasso__alpha': np.linspace(0.004, 0.01, 20)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [48], 'lasso__alpha': [0.008736842105263157]}\n",
    "Train RMSE: 0.170, R2: 0.324\n",
    "Test RMSE: 0.170, R2: 0.246\n",
    "Working on fold 2\n",
    "{'pca__n_components': [48, 65], 'lasso__alpha': [0.008736842105263157, 0.01]}\n",
    "Train RMSE: 0.166, R2: 0.354\n",
    "Test RMSE: 0.194, R2: 0.045\n",
    "Working on fold 3\n",
    "{'pca__n_components': [48, 65, 48], 'lasso__alpha': [0.008736842105263157, 0.01, 0.004]}\n",
    "Train RMSE: 0.163, R2: 0.360\n",
    "Test RMSE: 0.182, R2: 0.242\n",
    "Working on fold 4\n",
    "{'pca__n_components': [48, 65, 48, 77], 'lasso__alpha': [0.008736842105263157, 0.01, 0.004, 0.008105263157894737]}\n",
    "Train RMSE: 0.162, R2: 0.360\n",
    "Test RMSE: 0.176, R2: 0.313\n",
    "Working on fold 5\n",
    "{'pca__n_components': [48, 65, 48, 77, 67], 'lasso__alpha': [0.008736842105263157, 0.01, 0.004, 0.008105263157894737, 0.009368421052631578]}\n",
    "Train RMSE: 0.164, R2: 0.359\n",
    "Test RMSE: 0.179, R2: 0.254\n",
    "Average Train RMSE: 0.165\n",
    "Average Train R2: 0.351\n",
    "Average Test RMSE: 0.180\n",
    "Average Test R2: 0.220\n",
    "\n",
    "\n",
    "7. Performance (Best)\n",
    "\n",
    "'pca__n_components': np.linspace(40, 80, 20, dtype=int),\n",
    "'lasso__alpha': np.linspace(0.008, 0.009, 20)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [48], 'lasso__alpha': [0.008684210526315789]}\n",
    "Train RMSE: 0.170, R2: 0.324\n",
    "Test RMSE: 0.170, R2: 0.246\n",
    "Working on fold 2\n",
    "{'pca__n_components': [48, 65], 'lasso__alpha': [0.008684210526315789, 0.009]}\n",
    "Train RMSE: 0.165, R2: 0.365\n",
    "Test RMSE: 0.194, R2: 0.048\n",
    "Working on fold 3\n",
    "{'pca__n_components': [48, 65, 54], 'lasso__alpha': [0.008684210526315789, 0.009, 0.008]}\n",
    "Train RMSE: 0.165, R2: 0.342\n",
    "Test RMSE: 0.182, R2: 0.241\n",
    "Working on fold 4\n",
    "{'pca__n_components': [48, 65, 54, 77], 'lasso__alpha': [0.008684210526315789, 0.009, 0.008, 0.008]}\n",
    "Train RMSE: 0.162, R2: 0.362\n",
    "Test RMSE: 0.176, R2: 0.313\n",
    "Working on fold 5\n",
    "{'pca__n_components': [48, 65, 54, 77, 67], 'lasso__alpha': [0.008684210526315789, 0.009, 0.008, 0.008, 0.009]}\n",
    "Train RMSE: 0.163, R2: 0.363\n",
    "Test RMSE: 0.179, R2: 0.254\n",
    "Average Train RMSE: 0.165\n",
    "Average Train R2: 0.351\n",
    "Average Test RMSE: 0.180\n",
    "Average Test R2: 0.220\n",
    "\n",
    "\n",
    "Best Performance with correlation threshold set at 0.15\n",
    "\n",
    "'pca__n_components': np.linspace(40, 80, 20, dtype=int),\n",
    "'lasso__alpha': np.linspace(0.008, 0.009, 20)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [65], 'lasso__alpha': [0.009]}\n",
    "Train RMSE: 0.160, R2: 0.401\n",
    "Test RMSE: 0.172, R2: 0.224\n",
    "Working on fold 2\n",
    "{'pca__n_components': [65, 75], 'lasso__alpha': [0.009, 0.009]}\n",
    "Train RMSE: 0.156, R2: 0.430\n",
    "Test RMSE: 0.196, R2: 0.029\n",
    "Working on fold 3\n",
    "{'pca__n_components': [65, 75, 54], 'lasso__alpha': [0.009, 0.009, 0.009]}\n",
    "Train RMSE: 0.163, R2: 0.358\n",
    "Test RMSE: 0.182, R2: 0.243\n",
    "Working on fold 4\n",
    "{'pca__n_components': [65, 75, 54, 80], 'lasso__alpha': [0.009, 0.009, 0.009, 0.009]}\n",
    "Train RMSE: 0.157, R2: 0.402\n",
    "Test RMSE: 0.172, R2: 0.348\n",
    "Working on fold 5\n",
    "{'pca__n_components': [65, 75, 54, 80, 65], 'lasso__alpha': [0.009, 0.009, 0.009, 0.009, 0.009]}\n",
    "Train RMSE: 0.161, R2: 0.376\n",
    "Test RMSE: 0.176, R2: 0.275\n",
    "Average Train RMSE: 0.160\n",
    "Average Train R2: 0.393\n",
    "Average Test RMSE: 0.180\n",
    "Average Test R2: 0.224\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fold 1\n",
      "{'pca__n_components': [47], 'ridge__alpha': [600.0]}\n",
      "Train RMSE: 0.164, R2: 0.374\n",
      "Test RMSE: 0.173, R2: 0.220\n",
      "Working on fold 2\n",
      "{'pca__n_components': [47, 74], 'ridge__alpha': [600.0, 600.0]}\n",
      "Train RMSE: 0.152, R2: 0.455\n",
      "Test RMSE: 0.191, R2: 0.079\n",
      "Working on fold 3\n",
      "{'pca__n_components': [47, 74, 74], 'ridge__alpha': [600.0, 600.0, 600.0]}\n",
      "Train RMSE: 0.157, R2: 0.407\n",
      "Test RMSE: 0.178, R2: 0.271\n",
      "Working on fold 4\n",
      "{'pca__n_components': [47, 74, 74, 79], 'ridge__alpha': [600.0, 600.0, 600.0, 600.0]}\n",
      "Train RMSE: 0.158, R2: 0.390\n",
      "Test RMSE: 0.174, R2: 0.328\n",
      "Working on fold 5\n",
      "{'pca__n_components': [47, 74, 74, 79, 90], 'ridge__alpha': [600.0, 600.0, 600.0, 600.0, 600.0]}\n",
      "Train RMSE: 0.152, R2: 0.448\n",
      "Test RMSE: 0.174, R2: 0.290\n",
      "Average Train RMSE: 0.157\n",
      "Average Train R2: 0.415\n",
      "Average Test RMSE: 0.178\n",
      "Average Test R2: 0.238\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function for Ridge Regression (PCA as additional Feature Selection step)\n",
    "def evaluation(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    rmse = mean_squared_error(y, y_pred, squared=False)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    return rmse, r2\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "fold = 1\n",
    "\n",
    "train_metrics = {'rmse': [], 'r2': []}\n",
    "test_metrics = {'rmse': [], 'r2': []}\n",
    "best_params = {'pca__n_components': [], 'ridge__alpha': []}\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {'pca__n_components': np.linspace(40, 90, 20, dtype=int),\n",
    "              'ridge__alpha': np.linspace(200, 600, 20)}\n",
    "\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    print(f\"Working on fold {fold}\")\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('pca', PCA(random_state=0)),\n",
    "                     ('ridge', Ridge())])\n",
    "    \n",
    "    search = GridSearchCV(pipe, param_grid, n_jobs=4)\n",
    "    clf = search.fit(X_train, y_train)\n",
    "\n",
    "    best_params['pca__n_components'].append(search.best_params_['pca__n_components'])\n",
    "    best_params['ridge__alpha'].append(search.best_params_['ridge__alpha'])\n",
    "    print(best_params)\n",
    "\n",
    "    train_eval = evaluation(clf, X_train, y_train)\n",
    "    train_metrics['rmse'].append(train_eval[0])\n",
    "    train_metrics['r2'].append(train_eval[1])\n",
    "    \n",
    "    test_eval = evaluation(clf, X_test, y_test)\n",
    "    test_metrics['rmse'].append(test_eval[0])\n",
    "    test_metrics['r2'].append(test_eval[1])\n",
    "\n",
    "    print(f\"Train RMSE: {train_eval[0]:.3f}, R2: {train_eval[1]:.3f}\")\n",
    "    print(f\"Test RMSE: {test_eval[0]:.3f}, R2: {test_eval[1]:.3f}\")\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "\n",
    "\n",
    "# Print average metrics\n",
    "print(\"Average Train RMSE: {:.3f}\".format(np.mean(train_metrics['rmse'])))\n",
    "print(\"Average Train R2: {:.3f}\".format(np.mean(train_metrics['r2'])))\n",
    "print(\"Average Test RMSE: {:.3f}\".format(np.mean(test_metrics['rmse'])))\n",
    "print(\"Average Test R2: {:.3f}\".format(np.mean(test_metrics['r2'])))\n",
    "\n",
    "summary.loc['Ridge_Regression']['Average_Train_R2'] = np.mean(train_metrics['r2'])\n",
    "summary.loc['Ridge_Regression']['Average_Test_R2'] = np.mean(test_metrics['r2'])\n",
    "summary.loc['Ridge_Regression']['Average_Train_RMSE'] = np.mean(train_metrics['rmse'])\n",
    "summary.loc['Ridge_Regression']['Average_Test_RMSE'] = np.mean(test_metrics['rmse'])\n",
    "\n",
    "summary.loc['Ridge_Regression']['Train_R2_STD'] = np.std(train_metrics['r2'])\n",
    "summary.loc['Ridge_Regression']['Test_R2_STD'] = np.std(test_metrics['r2'])\n",
    "summary.loc['Ridge_Regression']['Train_RMSE_STD'] = np.std(train_metrics['rmse'])\n",
    "summary.loc['Ridge_Regression']['Test_RMSE_STD'] = np.std(test_metrics['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1. Performance\\n\\n'pca__n_components': np.linspace(5, 100, 10, dtype=int)\\n'ridge__alpha': np.logspace(-4, 4, 20)\\n\\nWorking on fold 1\\n{'pca__n_components': [47], 'ridge__alpha': [206.913808111479]}\\nTrain RMSE: 0.167, R2: 0.350\\nTest RMSE: 0.168, R2: 0.263\\nWorking on fold 2\\n{'pca__n_components': [47, 68], 'ridge__alpha': [206.913808111479, 545.5594781168514]}\\nTrain RMSE: 0.162, R2: 0.382\\nTest RMSE: 0.193, R2: 0.055\\nWorking on fold 3\\n{'pca__n_components': [47, 68, 47], 'ridge__alpha': [206.913808111479, 545.5594781168514, 206.913808111479]}\\nTrain RMSE: 0.163, R2: 0.356\\nTest RMSE: 0.181, R2: 0.250\\nWorking on fold 4\\n{'pca__n_components': [47, 68, 47, 89], 'ridge__alpha': [206.913808111479, 545.5594781168514, 206.913808111479, 545.5594781168514]}\\nTrain RMSE: 0.162, R2: 0.358\\nTest RMSE: 0.179, R2: 0.293\\nWorking on fold 5\\n{'pca__n_components': [47, 68, 47, 89, 100], 'ridge__alpha': [206.913808111479, 545.5594781168514, 206.913808111479, 545.5594781168514, 545.5594781168514]}\\nTrain RMSE: 0.161, R2: 0.383\\nTest RMSE: 0.178, R2: 0.258\\nAverage Train RMSE: 0.163\\nAverage Train R2: 0.366\\nAverage Test RMSE: 0.180\\nAverage Test R2: 0.224\\n\\n\\n2. Performance\\n\\n'pca__n_components': np.linspace(47, 90, 50, dtype=int)\\n'ridge__alpha': np.logspace(-4, 4, 50)\\n\\nWorking on fold 1\\n{'pca__n_components': [47], 'ridge__alpha': [339.3221771895323]}\\nTrain RMSE: 0.168, R2: 0.339\\nTest RMSE: 0.168, R2: 0.265\\nWorking on fold 2\\n{'pca__n_components': [47, 64], 'ridge__alpha': [339.3221771895323, 494.1713361323828]}\\nTrain RMSE: 0.162, R2: 0.381\\nTest RMSE: 0.194, R2: 0.047\\nWorking on fold 3\\n{'pca__n_components': [47, 64, 49], 'ridge__alpha': [339.3221771895323, 494.1713361323828, 232.99518105153672]}\\nTrain RMSE: 0.163, R2: 0.362\\nTest RMSE: 0.182, R2: 0.240\\nWorking on fold 4\\n{'pca__n_components': [47, 64, 49, 90], 'ridge__alpha': [339.3221771895323, 494.1713361323828, 232.99518105153672, 339.3221771895323]}\\nTrain RMSE: 0.158, R2: 0.391\\nTest RMSE: 0.179, R2: 0.294\\nWorking on fold 5\\n{'pca__n_components': [47, 64, 49, 90, 74], 'ridge__alpha': [339.3221771895323, 494.1713361323828, 232.99518105153672, 339.3221771895323, 494.1713361323828]}\\nTrain RMSE: 0.161, R2: 0.376\\nTest RMSE: 0.178, R2: 0.261\\nAverage Train RMSE: 0.163\\nAverage Train R2: 0.370\\nAverage Test RMSE: 0.180\\nAverage Test R2: 0.221\\n\\n\\n3. Performance\\n\\n'pca__n_components': np.linspace(5, 100, 10, dtype=int)\\n'ridge__alpha': np.logspace(2, 4, 20)\\n\\nWorking on fold 1\\n{'pca__n_components': [47], 'ridge__alpha': [335.9818286283781]}\\nTrain RMSE: 0.168, R2: 0.339\\nTest RMSE: 0.168, R2: 0.265\\nWorking on fold 2\\n{'pca__n_components': [47, 68], 'ridge__alpha': [335.9818286283781, 545.559478116852]}\\nTrain RMSE: 0.162, R2: 0.382\\nTest RMSE: 0.193, R2: 0.055\\nWorking on fold 3\\n{'pca__n_components': [47, 68, 47], 'ridge__alpha': [335.9818286283781, 545.559478116852, 263.6650898730358]}\\nTrain RMSE: 0.164, R2: 0.351\\nTest RMSE: 0.181, R2: 0.252\\nWorking on fold 4\\n{'pca__n_components': [47, 68, 47, 89], 'ridge__alpha': [335.9818286283781, 545.559478116852, 263.6650898730358, 428.13323987193957]}\\nTrain RMSE: 0.160, R2: 0.374\\nTest RMSE: 0.179, R2: 0.292\\nWorking on fold 5\\n{'pca__n_components': [47, 68, 47, 89, 100], 'ridge__alpha': [335.9818286283781, 545.559478116852, 263.6650898730358, 428.13323987193957, 545.559478116852]}\\nTrain RMSE: 0.161, R2: 0.383\\nTest RMSE: 0.178, R2: 0.258\\nAverage Train RMSE: 0.163\\nAverage Train R2: 0.366\\nAverage Test RMSE: 0.180\\nAverage Test R2: 0.224\\n\\n\\n4. Performance\\n\\n'pca__n_components': np.linspace(5, 100, 10, dtype=int)\\n'ridge__alpha': np.logspace(2, 3, 20)\\n\\nWorking on fold 1\\n{'pca__n_components': [47], 'ridge__alpha': [297.63514416313194]}\\nTrain RMSE: 0.168, R2: 0.342\\nTest RMSE: 0.168, R2: 0.264\\nWorking on fold 2\\n{'pca__n_components': [47, 68], 'ridge__alpha': [297.63514416313194, 483.2930238571752]}\\nTrain RMSE: 0.161, R2: 0.388\\nTest RMSE: 0.193, R2: 0.056\\nWorking on fold 3\\n{'pca__n_components': [47, 68, 47], 'ridge__alpha': [297.63514416313194, 483.2930238571752, 233.57214690901213]}\\nTrain RMSE: 0.164, R2: 0.354\\nTest RMSE: 0.181, R2: 0.251\\nWorking on fold 4\\n{'pca__n_components': [47, 68, 47, 89], 'ridge__alpha': [297.63514416313194, 483.2930238571752, 233.57214690901213, 379.26901907322497]}\\nTrain RMSE: 0.159, R2: 0.382\\nTest RMSE: 0.179, R2: 0.291\\nWorking on fold 5\\n{'pca__n_components': [47, 68, 47, 89, 100], 'ridge__alpha': [297.63514416313194, 483.2930238571752, 233.57214690901213, 379.26901907322497, 545.559478116852]}\\nTrain RMSE: 0.161, R2: 0.383\\nTest RMSE: 0.178, R2: 0.258\\nAverage Train RMSE: 0.163\\nAverage Train R2: 0.370\\nAverage Test RMSE: 0.180\\nAverage Test R2: 0.224\\n\\n\\n5. Performance (Best)\\n\\n'pca__n_components': np.linspace(40, 90, 20, dtype=int)\\n'ridge__alpha': np.linspace(200, 600, 20)\\n\\nWorking on fold 1\\n{'pca__n_components': [47], 'ridge__alpha': [305.2631578947369]}\\nTrain RMSE: 0.168, R2: 0.342\\nTest RMSE: 0.168, R2: 0.264\\nWorking on fold 2\\n{'pca__n_components': [47, 68], 'ridge__alpha': [305.2631578947369, 515.7894736842105]}\\nTrain RMSE: 0.162, R2: 0.385\\nTest RMSE: 0.193, R2: 0.056\\nWorking on fold 3\\n{'pca__n_components': [47, 68, 47], 'ridge__alpha': [305.2631578947369, 515.7894736842105, 242.10526315789474]}\\nTrain RMSE: 0.164, R2: 0.353\\nTest RMSE: 0.181, R2: 0.252\\nWorking on fold 4\\n{'pca__n_components': [47, 68, 47, 90], 'ridge__alpha': [305.2631578947369, 515.7894736842105, 242.10526315789474, 389.47368421052636]}\\nTrain RMSE: 0.159, R2: 0.382\\nTest RMSE: 0.179, R2: 0.295\\nWorking on fold 5\\n{'pca__n_components': [47, 68, 47, 90, 74], 'ridge__alpha': [305.2631578947369, 515.7894736842105, 242.10526315789474, 389.47368421052636, 473.68421052631584]}\\nTrain RMSE: 0.161, R2: 0.378\\nTest RMSE: 0.178, R2: 0.261\\nAverage Train RMSE: 0.163\\nAverage Train R2: 0.368\\nAverage Test RMSE: 0.180\\nAverage Test R2: 0.225\\n\\n\\n6. Performance\\n\\n'pca__n_components': np.linspace(40, 90, 20, dtype=int)\\n'ridge__alpha': np.linspace(400, 500, 20)\\n\\nWorking on fold 1\\n{'pca__n_components': [47], 'ridge__alpha': [400.0]}\\nTrain RMSE: 0.169, R2: 0.335\\nTest RMSE: 0.168, R2: 0.265\\nWorking on fold 2\\n{'pca__n_components': [47, 68], 'ridge__alpha': [400.0, 500.0]}\\nTrain RMSE: 0.162, R2: 0.387\\nTest RMSE: 0.193, R2: 0.056\\nWorking on fold 3\\n{'pca__n_components': [47, 68, 50], 'ridge__alpha': [400.0, 500.0, 400.0]}\\nTrain RMSE: 0.165, R2: 0.346\\nTest RMSE: 0.181, R2: 0.246\\nWorking on fold 4\\n{'pca__n_components': [47, 68, 50, 90], 'ridge__alpha': [400.0, 500.0, 400.0, 400.0]}\\nTrain RMSE: 0.160, R2: 0.381\\nTest RMSE: 0.179, R2: 0.295\\nWorking on fold 5\\n{'pca__n_components': [47, 68, 50, 90, 74], 'ridge__alpha': [400.0, 500.0, 400.0, 400.0, 463.1578947368421]}\\nTrain RMSE: 0.161, R2: 0.380\\nTest RMSE: 0.178, R2: 0.260\\nAverage Train RMSE: 0.163\\nAverage Train R2: 0.366\\nAverage Test RMSE: 0.180\\nAverage Test R2: 0.224\\n\\n\""
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Performance\n",
    "\n",
    "'pca__n_components': np.linspace(5, 100, 10, dtype=int)\n",
    "'ridge__alpha': np.logspace(-4, 4, 20)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [47], 'ridge__alpha': [206.913808111479]}\n",
    "Train RMSE: 0.167, R2: 0.350\n",
    "Test RMSE: 0.168, R2: 0.263\n",
    "Working on fold 2\n",
    "{'pca__n_components': [47, 68], 'ridge__alpha': [206.913808111479, 545.5594781168514]}\n",
    "Train RMSE: 0.162, R2: 0.382\n",
    "Test RMSE: 0.193, R2: 0.055\n",
    "Working on fold 3\n",
    "{'pca__n_components': [47, 68, 47], 'ridge__alpha': [206.913808111479, 545.5594781168514, 206.913808111479]}\n",
    "Train RMSE: 0.163, R2: 0.356\n",
    "Test RMSE: 0.181, R2: 0.250\n",
    "Working on fold 4\n",
    "{'pca__n_components': [47, 68, 47, 89], 'ridge__alpha': [206.913808111479, 545.5594781168514, 206.913808111479, 545.5594781168514]}\n",
    "Train RMSE: 0.162, R2: 0.358\n",
    "Test RMSE: 0.179, R2: 0.293\n",
    "Working on fold 5\n",
    "{'pca__n_components': [47, 68, 47, 89, 100], 'ridge__alpha': [206.913808111479, 545.5594781168514, 206.913808111479, 545.5594781168514, 545.5594781168514]}\n",
    "Train RMSE: 0.161, R2: 0.383\n",
    "Test RMSE: 0.178, R2: 0.258\n",
    "Average Train RMSE: 0.163\n",
    "Average Train R2: 0.366\n",
    "Average Test RMSE: 0.180\n",
    "Average Test R2: 0.224\n",
    "\n",
    "\n",
    "2. Performance\n",
    "\n",
    "'pca__n_components': np.linspace(47, 90, 50, dtype=int)\n",
    "'ridge__alpha': np.logspace(-4, 4, 50)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [47], 'ridge__alpha': [339.3221771895323]}\n",
    "Train RMSE: 0.168, R2: 0.339\n",
    "Test RMSE: 0.168, R2: 0.265\n",
    "Working on fold 2\n",
    "{'pca__n_components': [47, 64], 'ridge__alpha': [339.3221771895323, 494.1713361323828]}\n",
    "Train RMSE: 0.162, R2: 0.381\n",
    "Test RMSE: 0.194, R2: 0.047\n",
    "Working on fold 3\n",
    "{'pca__n_components': [47, 64, 49], 'ridge__alpha': [339.3221771895323, 494.1713361323828, 232.99518105153672]}\n",
    "Train RMSE: 0.163, R2: 0.362\n",
    "Test RMSE: 0.182, R2: 0.240\n",
    "Working on fold 4\n",
    "{'pca__n_components': [47, 64, 49, 90], 'ridge__alpha': [339.3221771895323, 494.1713361323828, 232.99518105153672, 339.3221771895323]}\n",
    "Train RMSE: 0.158, R2: 0.391\n",
    "Test RMSE: 0.179, R2: 0.294\n",
    "Working on fold 5\n",
    "{'pca__n_components': [47, 64, 49, 90, 74], 'ridge__alpha': [339.3221771895323, 494.1713361323828, 232.99518105153672, 339.3221771895323, 494.1713361323828]}\n",
    "Train RMSE: 0.161, R2: 0.376\n",
    "Test RMSE: 0.178, R2: 0.261\n",
    "Average Train RMSE: 0.163\n",
    "Average Train R2: 0.370\n",
    "Average Test RMSE: 0.180\n",
    "Average Test R2: 0.221\n",
    "\n",
    "\n",
    "3. Performance\n",
    "\n",
    "'pca__n_components': np.linspace(5, 100, 10, dtype=int)\n",
    "'ridge__alpha': np.logspace(2, 4, 20)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [47], 'ridge__alpha': [335.9818286283781]}\n",
    "Train RMSE: 0.168, R2: 0.339\n",
    "Test RMSE: 0.168, R2: 0.265\n",
    "Working on fold 2\n",
    "{'pca__n_components': [47, 68], 'ridge__alpha': [335.9818286283781, 545.559478116852]}\n",
    "Train RMSE: 0.162, R2: 0.382\n",
    "Test RMSE: 0.193, R2: 0.055\n",
    "Working on fold 3\n",
    "{'pca__n_components': [47, 68, 47], 'ridge__alpha': [335.9818286283781, 545.559478116852, 263.6650898730358]}\n",
    "Train RMSE: 0.164, R2: 0.351\n",
    "Test RMSE: 0.181, R2: 0.252\n",
    "Working on fold 4\n",
    "{'pca__n_components': [47, 68, 47, 89], 'ridge__alpha': [335.9818286283781, 545.559478116852, 263.6650898730358, 428.13323987193957]}\n",
    "Train RMSE: 0.160, R2: 0.374\n",
    "Test RMSE: 0.179, R2: 0.292\n",
    "Working on fold 5\n",
    "{'pca__n_components': [47, 68, 47, 89, 100], 'ridge__alpha': [335.9818286283781, 545.559478116852, 263.6650898730358, 428.13323987193957, 545.559478116852]}\n",
    "Train RMSE: 0.161, R2: 0.383\n",
    "Test RMSE: 0.178, R2: 0.258\n",
    "Average Train RMSE: 0.163\n",
    "Average Train R2: 0.366\n",
    "Average Test RMSE: 0.180\n",
    "Average Test R2: 0.224\n",
    "\n",
    "\n",
    "4. Performance\n",
    "\n",
    "'pca__n_components': np.linspace(5, 100, 10, dtype=int)\n",
    "'ridge__alpha': np.logspace(2, 3, 20)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [47], 'ridge__alpha': [297.63514416313194]}\n",
    "Train RMSE: 0.168, R2: 0.342\n",
    "Test RMSE: 0.168, R2: 0.264\n",
    "Working on fold 2\n",
    "{'pca__n_components': [47, 68], 'ridge__alpha': [297.63514416313194, 483.2930238571752]}\n",
    "Train RMSE: 0.161, R2: 0.388\n",
    "Test RMSE: 0.193, R2: 0.056\n",
    "Working on fold 3\n",
    "{'pca__n_components': [47, 68, 47], 'ridge__alpha': [297.63514416313194, 483.2930238571752, 233.57214690901213]}\n",
    "Train RMSE: 0.164, R2: 0.354\n",
    "Test RMSE: 0.181, R2: 0.251\n",
    "Working on fold 4\n",
    "{'pca__n_components': [47, 68, 47, 89], 'ridge__alpha': [297.63514416313194, 483.2930238571752, 233.57214690901213, 379.26901907322497]}\n",
    "Train RMSE: 0.159, R2: 0.382\n",
    "Test RMSE: 0.179, R2: 0.291\n",
    "Working on fold 5\n",
    "{'pca__n_components': [47, 68, 47, 89, 100], 'ridge__alpha': [297.63514416313194, 483.2930238571752, 233.57214690901213, 379.26901907322497, 545.559478116852]}\n",
    "Train RMSE: 0.161, R2: 0.383\n",
    "Test RMSE: 0.178, R2: 0.258\n",
    "Average Train RMSE: 0.163\n",
    "Average Train R2: 0.370\n",
    "Average Test RMSE: 0.180\n",
    "Average Test R2: 0.224\n",
    "\n",
    "\n",
    "5. Performance (Best)\n",
    "\n",
    "'pca__n_components': np.linspace(40, 90, 20, dtype=int)\n",
    "'ridge__alpha': np.linspace(200, 600, 20)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [47], 'ridge__alpha': [305.2631578947369]}\n",
    "Train RMSE: 0.168, R2: 0.342\n",
    "Test RMSE: 0.168, R2: 0.264\n",
    "Working on fold 2\n",
    "{'pca__n_components': [47, 68], 'ridge__alpha': [305.2631578947369, 515.7894736842105]}\n",
    "Train RMSE: 0.162, R2: 0.385\n",
    "Test RMSE: 0.193, R2: 0.056\n",
    "Working on fold 3\n",
    "{'pca__n_components': [47, 68, 47], 'ridge__alpha': [305.2631578947369, 515.7894736842105, 242.10526315789474]}\n",
    "Train RMSE: 0.164, R2: 0.353\n",
    "Test RMSE: 0.181, R2: 0.252\n",
    "Working on fold 4\n",
    "{'pca__n_components': [47, 68, 47, 90], 'ridge__alpha': [305.2631578947369, 515.7894736842105, 242.10526315789474, 389.47368421052636]}\n",
    "Train RMSE: 0.159, R2: 0.382\n",
    "Test RMSE: 0.179, R2: 0.295\n",
    "Working on fold 5\n",
    "{'pca__n_components': [47, 68, 47, 90, 74], 'ridge__alpha': [305.2631578947369, 515.7894736842105, 242.10526315789474, 389.47368421052636, 473.68421052631584]}\n",
    "Train RMSE: 0.161, R2: 0.378\n",
    "Test RMSE: 0.178, R2: 0.261\n",
    "Average Train RMSE: 0.163\n",
    "Average Train R2: 0.368\n",
    "Average Test RMSE: 0.180\n",
    "Average Test R2: 0.225\n",
    "\n",
    "\n",
    "6. Performance\n",
    "\n",
    "'pca__n_components': np.linspace(40, 90, 20, dtype=int)\n",
    "'ridge__alpha': np.linspace(400, 500, 20)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [47], 'ridge__alpha': [400.0]}\n",
    "Train RMSE: 0.169, R2: 0.335\n",
    "Test RMSE: 0.168, R2: 0.265\n",
    "Working on fold 2\n",
    "{'pca__n_components': [47, 68], 'ridge__alpha': [400.0, 500.0]}\n",
    "Train RMSE: 0.162, R2: 0.387\n",
    "Test RMSE: 0.193, R2: 0.056\n",
    "Working on fold 3\n",
    "{'pca__n_components': [47, 68, 50], 'ridge__alpha': [400.0, 500.0, 400.0]}\n",
    "Train RMSE: 0.165, R2: 0.346\n",
    "Test RMSE: 0.181, R2: 0.246\n",
    "Working on fold 4\n",
    "{'pca__n_components': [47, 68, 50, 90], 'ridge__alpha': [400.0, 500.0, 400.0, 400.0]}\n",
    "Train RMSE: 0.160, R2: 0.381\n",
    "Test RMSE: 0.179, R2: 0.295\n",
    "Working on fold 5\n",
    "{'pca__n_components': [47, 68, 50, 90, 74], 'ridge__alpha': [400.0, 500.0, 400.0, 400.0, 463.1578947368421]}\n",
    "Train RMSE: 0.161, R2: 0.380\n",
    "Test RMSE: 0.178, R2: 0.260\n",
    "Average Train RMSE: 0.163\n",
    "Average Train R2: 0.366\n",
    "Average Test RMSE: 0.180\n",
    "Average Test R2: 0.224\n",
    "\n",
    "\n",
    "\n",
    "Best Performance with correlation threshold at 0.15\n",
    "\n",
    "'pca__n_components': np.linspace(40, 90, 20, dtype=int),\n",
    "'ridge__alpha': np.linspace(200, 600, 20)\n",
    "\n",
    "Working on fold 1\n",
    "{'pca__n_components': [47], 'ridge__alpha': [600.0]}\n",
    "Train RMSE: 0.164, R2: 0.374\n",
    "Test RMSE: 0.173, R2: 0.220\n",
    "Working on fold 2\n",
    "{'pca__n_components': [47, 74], 'ridge__alpha': [600.0, 600.0]}\n",
    "Train RMSE: 0.152, R2: 0.455\n",
    "Test RMSE: 0.191, R2: 0.079\n",
    "Working on fold 3\n",
    "{'pca__n_components': [47, 74, 74], 'ridge__alpha': [600.0, 600.0, 600.0]}\n",
    "Train RMSE: 0.157, R2: 0.407\n",
    "Test RMSE: 0.178, R2: 0.271\n",
    "Working on fold 4\n",
    "{'pca__n_components': [47, 74, 74, 79], 'ridge__alpha': [600.0, 600.0, 600.0, 600.0]}\n",
    "Train RMSE: 0.158, R2: 0.390\n",
    "Test RMSE: 0.174, R2: 0.328\n",
    "Working on fold 5\n",
    "{'pca__n_components': [47, 74, 74, 79, 90], 'ridge__alpha': [600.0, 600.0, 600.0, 600.0, 600.0]}\n",
    "Train RMSE: 0.152, R2: 0.448\n",
    "Test RMSE: 0.174, R2: 0.290\n",
    "Average Train RMSE: 0.157\n",
    "Average Train R2: 0.415\n",
    "Average Test RMSE: 0.178\n",
    "Average Test R2: 0.238\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average_Test_R2</th>\n",
       "      <th>Test_R2_STD</th>\n",
       "      <th>Average_Test_RMSE</th>\n",
       "      <th>Test_RMSE_STD</th>\n",
       "      <th>Average_Train_R2</th>\n",
       "      <th>Train_R2_STD</th>\n",
       "      <th>Average_Train_RMSE</th>\n",
       "      <th>Train_RMSE_STD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear_Regression</th>\n",
       "      <td>0.196089</td>\n",
       "      <td>0.109464</td>\n",
       "      <td>0.182699</td>\n",
       "      <td>0.007987</td>\n",
       "      <td>0.330911</td>\n",
       "      <td>0.032419</td>\n",
       "      <td>0.167496</td>\n",
       "      <td>0.002804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso_Regression</th>\n",
       "      <td>0.224038</td>\n",
       "      <td>0.10606</td>\n",
       "      <td>0.17953</td>\n",
       "      <td>0.008751</td>\n",
       "      <td>0.393388</td>\n",
       "      <td>0.024672</td>\n",
       "      <td>0.159513</td>\n",
       "      <td>0.002813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge_Regression</th>\n",
       "      <td>0.237583</td>\n",
       "      <td>0.086823</td>\n",
       "      <td>0.178102</td>\n",
       "      <td>0.00647</td>\n",
       "      <td>0.414903</td>\n",
       "      <td>0.031707</td>\n",
       "      <td>0.156644</td>\n",
       "      <td>0.00436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Average_Test_R2 Test_R2_STD Average_Test_RMSE Test_RMSE_STD  \\\n",
       "Linear_Regression        0.196089    0.109464          0.182699      0.007987   \n",
       "Lasso_Regression         0.224038     0.10606           0.17953      0.008751   \n",
       "Ridge_Regression         0.237583    0.086823          0.178102       0.00647   \n",
       "\n",
       "                  Average_Train_R2 Train_R2_STD Average_Train_RMSE  \\\n",
       "Linear_Regression         0.330911     0.032419           0.167496   \n",
       "Lasso_Regression          0.393388     0.024672           0.159513   \n",
       "Ridge_Regression          0.414903     0.031707           0.156644   \n",
       "\n",
       "                  Train_RMSE_STD  \n",
       "Linear_Regression       0.002804  \n",
       "Lasso_Regression        0.002813  \n",
       "Ridge_Regression         0.00436  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regression_types = ['Linear_Regression', 'Lasso_Regression', 'Ridge_Regression']\n",
    "average_test_r2 = summary['Average_Test_R2']\n",
    "test_r2_std = summary['Test_R2_STD']\n",
    "average_test_rmse = summary['Average_Test_RMSE']\n",
    "test_rmse_std = summary['Test_RMSE_STD']\n",
    "average_train_r2 = summary['Average_Train_R2']\n",
    "train_r2_std = summary['Train_R2_STD']\n",
    "average_train_rmse = summary['Average_Train_RMSE']\n",
    "train_rmse_std = summary['Train_RMSE_STD']\n",
    "\n",
    "# Define the common range for R2 and RMSE\n",
    "r2_range = (0, max(max(average_test_r2 + test_r2_std), max(average_train_r2 + train_r2_std)) + 0.05)\n",
    "rmse_range = (0, max(max(average_test_rmse + test_rmse_std), max(average_train_rmse + train_rmse_std)) + 0.05)\n",
    "\n",
    "# Plot settings\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6), sharey=False)\n",
    "\n",
    "bar_width = 0.35  # Narrower bars\n",
    "index = np.arange(len(regression_types))  # Label locations\n",
    "\n",
    "# Average Test and Train R2\n",
    "axes[0].bar(index, average_train_r2, bar_width, yerr=train_r2_std, capsize=5, label='Train R2')\n",
    "axes[0].bar(index + bar_width, average_test_r2, bar_width, yerr=test_r2_std, capsize=5, label='Test R2')\n",
    "axes[0].set_title('R2 Scores')\n",
    "axes[0].set_ylabel('R2')\n",
    "axes[0].set_ylim(r2_range)\n",
    "axes[0].set_xticks(index + bar_width / 2)\n",
    "axes[0].set_xticklabels(regression_types)\n",
    "axes[0].legend()\n",
    "\n",
    "# Average Test and Train RMSE\n",
    "axes[1].bar(index, average_train_rmse, bar_width, yerr=train_rmse_std, capsize=5, label='Train RMSE')\n",
    "axes[1].bar(index + bar_width, average_test_rmse, bar_width, yerr=test_rmse_std, capsize=5, label='Test RMSE')\n",
    "axes[1].set_title('RMSE Scores')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].set_ylim(rmse_range)\n",
    "axes[1].set_xticks(index + bar_width / 2)\n",
    "axes[1].set_xticklabels(regression_types)\n",
    "axes[1].legend()\n",
    "\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../output/LR_results\")\n",
    "# Show plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
